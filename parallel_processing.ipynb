{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb2955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "from multiprocessing import Process, Queue, set_start_method\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ================= CONFIG =================\n",
    "main_input_dir = r\"D:\\NLPInsuranceProject\\AutonomousDrivingGanMV\\task1test\"\n",
    "output_video_path = \"final_output.mp4\"\n",
    "fps = 30\n",
    "\n",
    "obj_model_path = r\"C:\\Users\\DELL\\Documents\\GitHub\\AutonomousDrivingGanMV\\best.pt\"\n",
    "lane_model_path = r\"C:\\Users\\DELL\\Documents\\GitHub\\AutonomousDrivingGanMV\\ENET.pth\"\n",
    "sign_model_path = r\"C:\\Users\\DELL\\Documents\\GitHub\\AutonomousDrivingGanMV\\runsnano\\exp12\\weights\\best.pt\"\n",
    "# ==========================================\n",
    "\n",
    "def load_all_frames(main_dir):\n",
    "    frame_paths = []\n",
    "    for root, _, files in os.walk(main_dir):\n",
    "        image_files = sorted([f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "        for img in image_files:\n",
    "            frame_paths.append(os.path.join(root, img))\n",
    "    return frame_paths\n",
    "\n",
    "def run_yolo_inference(model, input_queue, output_queue, name=\"\"):\n",
    "    while True:\n",
    "        item = input_queue.get()\n",
    "        if item is None:\n",
    "            break\n",
    "        idx, frame = item\n",
    "        results = model.predict(source=frame, imgsz=640, conf=0.3, verbose=False)[0]\n",
    "        annotated = results.plot()\n",
    "        output_queue.put((idx, annotated))\n",
    "\n",
    "def merge_annotations(base_frame, annotated_frames):\n",
    "    merged = base_frame.copy()\n",
    "    for overlay in annotated_frames:\n",
    "        mask = overlay != 0\n",
    "        merged[mask] = overlay[mask]\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a75c461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading YOLO models and ENet lane segmentation model...\n",
      "[INFO] Models loaded and warmed up successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "\n",
    "# ====== CONFIG ======\n",
    "device = 'cpu'  # Change to 'cuda' for GPU acceleration\n",
    "obj_model_path = r\"C:\\Users\\DELL\\Documents\\GitHub\\AutonomousDrivingGanMV\\best.pt\"\n",
    "sign_model_path = r\"C:\\Users\\DELL\\Documents\\GitHub\\AutonomousDrivingGanMV\\runsnano\\exp12\\weights\\best.pt\"\n",
    "#lane_model_path = r\"C:\\Users\\DELL\\Documents\\GitHub\\AutonomousDrivingGanMV\\ENET.pth\"\n",
    "# =====================\n",
    "\n",
    "# === Load YOLO models ===\n",
    "def load_yolo_model(model_path):\n",
    "    model = YOLO(model_path)\n",
    "    return model\n",
    "\n",
    "# === Warmup ENet model (Lane Segmentation) ===\n",
    "\"\"\"def load_enet_model(model_path, device):\n",
    "    from models.enet import ENet  # Ensure the ENet model is defined in models/enet.py\n",
    "    model = ENet(12)  # Adjust num_classes if necessary\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    return model\"\"\"\n",
    "\n",
    "# Load models and warmup\n",
    "def warmup_models():\n",
    "    print(\"[INFO] Loading YOLO models and ENet lane segmentation model...\")\n",
    "    obj_model = load_yolo_model(obj_model_path)\n",
    "    sign_model = load_yolo_model(sign_model_path)\n",
    "    #lane_model = load_enet_model(lane_model_path, device)\n",
    "\n",
    "    # Warmup YOLO models with dummy frame\n",
    "    dummy_frame = np.zeros((640, 640, 3), dtype=np.uint8)\n",
    "    for model in [obj_model, sign_model]:\n",
    "        model.predict(source=dummy_frame, imgsz=640, conf=0.3, verbose=False)\n",
    "\n",
    "    print(\"[INFO] Models loaded and warmed up successfully!\")\n",
    "    return obj_model, sign_model    #, lane_model\n",
    "\n",
    "# Call the warmup function to load and prepare models\n",
    "obj_model, sign_model = warmup_models()  #, lane_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a084cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# === Config ===\n",
    "frame_folder = r\"C:\\Users\\Sejal Hanmante\\Downloads\\task1train540p\"  # Folder with frames\n",
    "frame_delay = 10  # ms delay between frames (adjust for speed)\n",
    "\n",
    "# === Use already-loaded models ===\n",
    "# Assuming obj_model and sign_model are already initialized in your current session.\n",
    "\n",
    "def process_frames_live_style(obj_model, sign_model, folder_path):\n",
    "    image_paths = sorted(Path(folder_path).glob(\"*.png\"))  # Modify if you use .png or .jpeg\n",
    "\n",
    "    if not image_paths:\n",
    "        print(\"[ERROR] No frames found in folder.\")\n",
    "        return\n",
    "\n",
    "    for img_path in image_paths:\n",
    "        frame = cv2.imread(str(img_path))\n",
    "        if frame is None:\n",
    "            print(f\"[Warning] Could not read {img_path}\")\n",
    "            continue\n",
    "\n",
    "        # Perform object detection\n",
    "        obj_results = obj_model.predict(source=frame, imgsz=640, conf=0.3, verbose=False)[0]\n",
    "        sign_results = sign_model.predict(source=frame, imgsz=640, conf=0.3, verbose=False)[0]\n",
    "\n",
    "        # Draw detections from both models\n",
    "        for result, color in zip([obj_results, sign_results], [(0, 255, 0), (0, 0, 255)]):\n",
    "            boxes = result.boxes\n",
    "            if boxes is not None:\n",
    "                for box in boxes:\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                    cls_id = int(box.cls[0])\n",
    "                    conf = float(box.conf[0])\n",
    "                    label = f\"{result.names[cls_id]} {conf:.2f}\"\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                    cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "        # Show the frame in a window\n",
    "        cv2.imshow(\"Live Detection - Press 'q' to Exit\", frame)\n",
    "        if cv2.waitKey(frame_delay) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# === Call the function using existing models ===\n",
    "process_frames_live_style(obj_model, sign_model, frame_folder)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79c6c77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "\n",
    "\n",
    "class LaneDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_path=\"E:/TuSimple/train_set\", train=True, size=(512, 256)):\n",
    "        self._dataset_path = dataset_path\n",
    "        self._mode = \"train\" if train else \"eval\"\n",
    "        self._image_size = size # w, h\n",
    "\n",
    "\n",
    "        if self._mode == \"train\":\n",
    "            label_files = [\n",
    "                os.path.join(self._dataset_path, f\"label_data_{suffix}.json\")\n",
    "                for suffix in (\"0313\", \"0531\")\n",
    "            ]\n",
    "        elif self._mode == \"eval\":\n",
    "            label_files = [\n",
    "                os.path.join(self._dataset_path, f\"label_data_{suffix}.json\")\n",
    "                for suffix in (\"0601\",)\n",
    "            ]\n",
    "\n",
    "        self._data = []\n",
    "\n",
    "        for label_file in label_files:\n",
    "            self._process_label_file(label_file)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self._dataset_path, self._data[idx][0])\n",
    "        image = cv2.imread(image_path)\n",
    "        h, w, c = image.shape\n",
    "        image = cv2.resize(image, self._image_size, interpolation=cv2.INTER_LINEAR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        image = image[..., None]\n",
    "        lanes = self._data[idx][1]\n",
    "\n",
    "        segmentation_image = self._draw(h, w, lanes, \"segmentation\")\n",
    "        instance_image = self._draw(h, w, lanes, \"instance\")\n",
    "\n",
    "        instance_image = instance_image[..., None]\n",
    "\n",
    "        image = torch.from_numpy(image).float().permute((2, 0, 1))\n",
    "        segmentation_image = torch.from_numpy(segmentation_image.copy())\n",
    "        instance_image =  torch.from_numpy(instance_image.copy()).permute((2, 0, 1))\n",
    "        segmentation_image = segmentation_image.to(torch.int64)\n",
    "\n",
    "        return image, segmentation_image, instance_image # 1 x H x W [[0, 1], [2, 0]]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def _draw(self, h, w, lanes, image_type):\n",
    "        image = np.zeros((h, w), dtype=np.uint8)\n",
    "        for i, lane in enumerate(lanes):\n",
    "            color = 1 if image_type == \"segmentation\" else i + 1\n",
    "            cv2.polylines(image, [lane], False, color, 10)\n",
    "\n",
    "        image = cv2.resize(image, self._image_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def _process_label_file(self, file_path):\n",
    "        with open(file_path) as f:\n",
    "            for line in f:\n",
    "                info = json.loads(line)\n",
    "                image = info[\"raw_file\"]\n",
    "                lanes = info[\"lanes\"]\n",
    "                h_samples = info[\"h_samples\"]\n",
    "                lanes_coords = []\n",
    "                for lane in lanes:\n",
    "                    x = np.array([lane]).T\n",
    "                    y = np.array([h_samples]).T\n",
    "                    xy = np.hstack((x, y))\n",
    "                    idx = np.where(xy[:, 0] > 0)\n",
    "                    lane_coords = xy[idx]\n",
    "                    lanes_coords.append(lane_coords)\n",
    "                self._data.append((image, lanes_coords))\n",
    "\n",
    "\n",
    "class InitialBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 bias=False,\n",
    "                 relu=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if relu:\n",
    "            activation = nn.ReLU\n",
    "        else:\n",
    "            activation = nn.PReLU\n",
    "\n",
    "        # Main branch - As stated above the number of output channels for this\n",
    "        # branch is the total minus 3, since the remaining channels come from\n",
    "        # the extension branch\n",
    "        self.main_branch = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels - 1,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            bias=bias)\n",
    "\n",
    "        # Extension branch\n",
    "        self.ext_branch = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "        # Initialize batch normalization to be used after concatenation\n",
    "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # PReLU layer to apply after concatenating the branches\n",
    "        self.out_activation = activation()\n",
    "\n",
    "    def forward(self, x):\n",
    "        main = self.main_branch(x)\n",
    "        ext = self.ext_branch(x)\n",
    "\n",
    "        # Concatenate branches\n",
    "        out = torch.cat((main, ext), 1)\n",
    "\n",
    "        # Apply batch normalization\n",
    "        out = self.batch_norm(out)\n",
    "\n",
    "        return self.out_activation(out)\n",
    "\n",
    "\n",
    "class RegularBottleneck(nn.Module):\n",
    "    def __init__(self,\n",
    "                 channels,\n",
    "                 internal_ratio=4,\n",
    "                 kernel_size=3,\n",
    "                 padding=0,\n",
    "                 dilation=1,\n",
    "                 asymmetric=False,\n",
    "                 dropout_prob=0,\n",
    "                 bias=False,\n",
    "                 relu=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Check in the internal_scale parameter is within the expected range\n",
    "        # [1, channels]\n",
    "        if internal_ratio <= 1 or internal_ratio > channels:\n",
    "            raise RuntimeError(\"Value out of range. Expected value in the \"\n",
    "                               \"interval [1, {0}], got internal_scale={1}.\"\n",
    "                               .format(channels, internal_ratio))\n",
    "\n",
    "        internal_channels = channels // internal_ratio\n",
    "\n",
    "        if relu:\n",
    "            activation = nn.ReLU\n",
    "        else:\n",
    "            activation = nn.PReLU\n",
    "\n",
    "        # Main branch - shortcut connection\n",
    "\n",
    "        # Extension branch - 1x1 convolution, followed by a regular, dilated or\n",
    "        # asymmetric convolution, followed by another 1x1 convolution, and,\n",
    "        # finally, a regularizer (spatial dropout). Number of channels is constant.\n",
    "\n",
    "        # 1x1 projection convolution\n",
    "        self.ext_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                channels,\n",
    "                internal_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                bias=bias), nn.BatchNorm2d(internal_channels), activation())\n",
    "\n",
    "        # If the convolution is asymmetric we split the main convolution in\n",
    "        # two. Eg. for a 5x5 asymmetric convolution we have two convolution:\n",
    "        # the first is 5x1 and the second is 1x5.\n",
    "        if asymmetric:\n",
    "            self.ext_conv2 = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    internal_channels,\n",
    "                    internal_channels,\n",
    "                    kernel_size=(kernel_size, 1),\n",
    "                    stride=1,\n",
    "                    padding=(padding, 0),\n",
    "                    dilation=dilation,\n",
    "                    bias=bias), nn.BatchNorm2d(internal_channels), activation(),\n",
    "                nn.Conv2d(\n",
    "                    internal_channels,\n",
    "                    internal_channels,\n",
    "                    kernel_size=(1, kernel_size),\n",
    "                    stride=1,\n",
    "                    padding=(0, padding),\n",
    "                    dilation=dilation,\n",
    "                    bias=bias), nn.BatchNorm2d(internal_channels), activation())\n",
    "        else:\n",
    "            self.ext_conv2 = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    internal_channels,\n",
    "                    internal_channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=1,\n",
    "                    padding=padding,\n",
    "                    dilation=dilation,\n",
    "                    bias=bias), nn.BatchNorm2d(internal_channels), activation())\n",
    "\n",
    "        # 1x1 expansion convolution\n",
    "        self.ext_conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                internal_channels,\n",
    "                channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                bias=bias), nn.BatchNorm2d(channels), activation())\n",
    "\n",
    "        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n",
    "\n",
    "        # PReLU layer to apply after adding the branches\n",
    "        self.out_activation = activation()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Main branch shortcut\n",
    "        main = x\n",
    "\n",
    "        # Extension branch\n",
    "        ext = self.ext_conv1(x)\n",
    "        ext = self.ext_conv2(ext)\n",
    "        ext = self.ext_conv3(ext)\n",
    "        ext = self.ext_regul(ext)\n",
    "\n",
    "        # Add main and extension branches\n",
    "        out = main + ext\n",
    "\n",
    "        return self.out_activation(out)\n",
    "\n",
    "\n",
    "class DownsamplingBottleneck(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 internal_ratio=4,\n",
    "                 return_indices=False,\n",
    "                 dropout_prob=0,\n",
    "                 bias=False,\n",
    "                 relu=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Store parameters that are needed later\n",
    "        self.return_indices = return_indices\n",
    "\n",
    "        # Check in the internal_scale parameter is within the expected range\n",
    "        # [1, channels]\n",
    "        if internal_ratio <= 1 or internal_ratio > in_channels:\n",
    "            raise RuntimeError(\"Value out of range. Expected value in the \"\n",
    "                               \"interval [1, {0}], got internal_scale={1}. \"\n",
    "                               .format(in_channels, internal_ratio))\n",
    "\n",
    "        internal_channels = in_channels // internal_ratio\n",
    "\n",
    "        if relu:\n",
    "            activation = nn.ReLU\n",
    "        else:\n",
    "            activation = nn.PReLU\n",
    "\n",
    "        # Main branch - max pooling followed by feature map (channels) padding\n",
    "        self.main_max1 = nn.MaxPool2d(\n",
    "            2,\n",
    "            stride=2,\n",
    "            return_indices=return_indices)\n",
    "\n",
    "        # Extension branch - 2x2 convolution, followed by a regular, dilated or\n",
    "        # asymmetric convolution, followed by another 1x1 convolution. Number\n",
    "        # of channels is doubled.\n",
    "\n",
    "        # 2x2 projection convolution with stride 2\n",
    "        self.ext_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                internal_channels,\n",
    "                kernel_size=2,\n",
    "                stride=2,\n",
    "                bias=bias), nn.BatchNorm2d(internal_channels), activation())\n",
    "\n",
    "        # Convolution\n",
    "        self.ext_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                internal_channels,\n",
    "                internal_channels,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=bias), nn.BatchNorm2d(internal_channels), activation())\n",
    "\n",
    "        # 1x1 expansion convolution\n",
    "        self.ext_conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                internal_channels,\n",
    "                out_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                bias=bias), nn.BatchNorm2d(out_channels), activation())\n",
    "\n",
    "        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n",
    "\n",
    "        # PReLU layer to apply after concatenating the branches\n",
    "        self.out_activation = activation()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Main branch shortcut\n",
    "        if self.return_indices:\n",
    "            main, max_indices = self.main_max1(x)\n",
    "        else:\n",
    "            main = self.main_max1(x)\n",
    "\n",
    "        # Extension branch\n",
    "        ext = self.ext_conv1(x)\n",
    "        ext = self.ext_conv2(ext)\n",
    "        ext = self.ext_conv3(ext)\n",
    "        ext = self.ext_regul(ext)\n",
    "\n",
    "        # Main branch channel padding\n",
    "        n, ch_ext, h, w = ext.size()\n",
    "        ch_main = main.size()[1]\n",
    "        padding = torch.zeros(n, ch_ext - ch_main, h, w)\n",
    "\n",
    "        # Before concatenating, check if main is on the CPU or GPU and\n",
    "        # convert padding accordingly\n",
    "        if main.is_cuda:\n",
    "            padding = padding.cuda()\n",
    "\n",
    "        # Concatenate\n",
    "        main = torch.cat((main, padding), 1)\n",
    "\n",
    "        # Add main and extension branches\n",
    "        out = main + ext\n",
    "\n",
    "        return self.out_activation(out), max_indices\n",
    "\n",
    "\n",
    "class UpsamplingBottleneck(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 internal_ratio=4,\n",
    "                 dropout_prob=0,\n",
    "                 bias=False,\n",
    "                 relu=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Check in the internal_scale parameter is within the expected range\n",
    "        # [1, channels]\n",
    "        if internal_ratio <= 1 or internal_ratio > in_channels:\n",
    "            raise RuntimeError(\"Value out of range. Expected value in the \"\n",
    "                               \"interval [1, {0}], got internal_scale={1}. \"\n",
    "                               .format(in_channels, internal_ratio))\n",
    "\n",
    "        internal_channels = in_channels // internal_ratio\n",
    "\n",
    "        if relu:\n",
    "            activation = nn.ReLU\n",
    "        else:\n",
    "            activation = nn.PReLU\n",
    "\n",
    "        # Main branch - max pooling followed by feature map (channels) padding\n",
    "        self.main_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias),\n",
    "            nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        # Remember that the stride is the same as the kernel_size, just like\n",
    "        # the max pooling layers\n",
    "        self.main_unpool1 = nn.MaxUnpool2d(kernel_size=2)\n",
    "\n",
    "        # Extension branch - 1x1 convolution, followed by a regular, dilated or\n",
    "        # asymmetric convolution, followed by another 1x1 convolution. Number\n",
    "        # of channels is doubled.\n",
    "\n",
    "        # 1x1 projection convolution with stride 1\n",
    "        self.ext_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels, internal_channels, kernel_size=1, bias=bias),\n",
    "            nn.BatchNorm2d(internal_channels), activation())\n",
    "\n",
    "        # Transposed convolution\n",
    "        self.ext_tconv1 = nn.ConvTranspose2d(\n",
    "            internal_channels,\n",
    "            internal_channels,\n",
    "            kernel_size=2,\n",
    "            stride=2,\n",
    "            bias=bias)\n",
    "        self.ext_tconv1_bnorm = nn.BatchNorm2d(internal_channels)\n",
    "        self.ext_tconv1_activation = activation()\n",
    "\n",
    "        # 1x1 expansion convolution\n",
    "        self.ext_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                internal_channels, out_channels, kernel_size=1, bias=bias),\n",
    "            nn.BatchNorm2d(out_channels), activation())\n",
    "\n",
    "        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n",
    "\n",
    "        # PReLU layer to apply after concatenating the branches\n",
    "        self.out_activation = activation()\n",
    "\n",
    "    def forward(self, x, max_indices, output_size):\n",
    "        # Main branch shortcut\n",
    "        main = self.main_conv1(x)\n",
    "        main = self.main_unpool1(\n",
    "            main, max_indices, output_size=output_size)\n",
    "\n",
    "        # Extension branch\n",
    "        ext = self.ext_conv1(x)\n",
    "        ext = self.ext_tconv1(ext, output_size=output_size)\n",
    "        ext = self.ext_tconv1_bnorm(ext)\n",
    "        ext = self.ext_tconv1_activation(ext)\n",
    "        ext = self.ext_conv2(ext)\n",
    "        ext = self.ext_regul(ext)\n",
    "\n",
    "        # Add main and extension branches\n",
    "        out = main + ext\n",
    "\n",
    "        return self.out_activation(out)\n",
    "\n",
    "\n",
    "class ENet(nn.Module):\n",
    "    def __init__(self, binary_seg, embedding_dim, encoder_relu=False, decoder_relu=True):\n",
    "        super(ENet, self).__init__()\n",
    "\n",
    "        self.initial_block = InitialBlock(1, 16, relu=encoder_relu)\n",
    "\n",
    "        # Stage 1 share\n",
    "        self.downsample1_0 = DownsamplingBottleneck(16, 64, return_indices=True, dropout_prob=0.01, relu=encoder_relu)\n",
    "        self.regular1_1 = RegularBottleneck(64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n",
    "        self.regular1_2 = RegularBottleneck(64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n",
    "        self.regular1_3 = RegularBottleneck(64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n",
    "        self.regular1_4 = RegularBottleneck(64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n",
    "\n",
    "        # Stage 2 share\n",
    "        self.downsample2_0 = DownsamplingBottleneck(64, 128, return_indices=True, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.regular2_1 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.dilated2_2 = RegularBottleneck(128, dilation=2, padding=2, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.asymmetric2_3 = RegularBottleneck(128, kernel_size=5, padding=2, asymmetric=True, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.dilated2_4 = RegularBottleneck(128, dilation=4, padding=4, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.regular2_5 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.dilated2_6 = RegularBottleneck(128, dilation=8, padding=8, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.asymmetric2_7 = RegularBottleneck(128, kernel_size=5, asymmetric=True, padding=2, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.dilated2_8 = RegularBottleneck(128, dilation=16, padding=16, dropout_prob=0.1, relu=encoder_relu)\n",
    "\n",
    "        # stage 3 binary\n",
    "        self.regular_binary_3_0 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.dilated_binary_3_1 = RegularBottleneck(128, dilation=2, padding=2, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.asymmetric_binary_3_2 = RegularBottleneck(128, kernel_size=5, padding=2, asymmetric=True, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.dilated_binary_3_3 = RegularBottleneck(128, dilation=4, padding=4, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.regular_binary_3_4 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.dilated_binary_3_5 = RegularBottleneck(128, dilation=8, padding=8, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.asymmetric_binary_3_6 = RegularBottleneck(128, kernel_size=5, asymmetric=True, padding=2, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.dilated_binary_3_7 = RegularBottleneck(128, dilation=16, padding=16, dropout_prob=0.1, relu=encoder_relu)\n",
    "\n",
    "        # stage 3 embedding\n",
    "        self.regular_embedding_3_0 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.dilated_embedding_3_1 = RegularBottleneck(128, dilation=2, padding=2, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.asymmetric_embedding_3_2 = RegularBottleneck(128, kernel_size=5, padding=2, asymmetric=True, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.dilated_embedding_3_3 = RegularBottleneck(128, dilation=4, padding=4, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.regular_embedding_3_4 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.dilated_embedding_3_5 = RegularBottleneck(128, dilation=8, padding=8, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.asymmetric_bembedding_3_6 = RegularBottleneck(128, kernel_size=5, asymmetric=True, padding=2, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.dilated_embedding_3_7 = RegularBottleneck(128, dilation=16, padding=16, dropout_prob=0.1, relu=encoder_relu)\n",
    "\n",
    "        # binary branch\n",
    "        self.upsample_binary_4_0 = UpsamplingBottleneck(128, 64, dropout_prob=0.1, relu=decoder_relu)\n",
    "        self.regular_binary_4_1 = RegularBottleneck(64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n",
    "        self.regular_binary_4_2 = RegularBottleneck(64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n",
    "        self.upsample_binary_5_0 = UpsamplingBottleneck(64, 16, dropout_prob=0.1, relu=decoder_relu)\n",
    "        self.regular_binary_5_1 = RegularBottleneck(16, padding=1, dropout_prob=0.1, relu=decoder_relu)\n",
    "        self.binary_transposed_conv = nn.ConvTranspose2d(16, binary_seg, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "\n",
    "        # embedding branch\n",
    "        self.upsample_embedding_4_0 = UpsamplingBottleneck(128, 64, dropout_prob=0.1, relu=decoder_relu)\n",
    "        self.regular_embedding_4_1 = RegularBottleneck(64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n",
    "        self.regular_embedding_4_2 = RegularBottleneck(64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n",
    "        self.upsample_embedding_5_0 = UpsamplingBottleneck(64, 16, dropout_prob=0.1, relu=decoder_relu)\n",
    "        self.regular_embedding_5_1 = RegularBottleneck(16, padding=1, dropout_prob=0.1, relu=decoder_relu)\n",
    "        self.embedding_transposed_conv = nn.ConvTranspose2d(16, embedding_dim, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial block\n",
    "        input_size = x.size()\n",
    "        x = self.initial_block(x)\n",
    "\n",
    "        # Stage 1 share\n",
    "        stage1_input_size = x.size()\n",
    "        x, max_indices1_0 = self.downsample1_0(x)\n",
    "        x = self.regular1_1(x)\n",
    "        x = self.regular1_2(x)\n",
    "        x = self.regular1_3(x)\n",
    "        x = self.regular1_4(x)\n",
    "\n",
    "        # Stage 2 share\n",
    "        stage2_input_size = x.size()\n",
    "        x, max_indices2_0 = self.downsample2_0(x)\n",
    "        x = self.regular2_1(x)\n",
    "        x = self.dilated2_2(x)\n",
    "        x = self.asymmetric2_3(x)\n",
    "        x = self.dilated2_4(x)\n",
    "        x = self.regular2_5(x)\n",
    "        x = self.dilated2_6(x)\n",
    "        x = self.asymmetric2_7(x)\n",
    "        x = self.dilated2_8(x)\n",
    "\n",
    "        # stage 3 binary\n",
    "        x_binary = self.regular_binary_3_0(x)\n",
    "        x_binary = self.dilated_binary_3_1(x_binary)\n",
    "        x_binary = self.asymmetric_binary_3_2(x_binary)\n",
    "        x_binary = self.dilated_binary_3_3(x_binary)\n",
    "        x_binary = self.regular_binary_3_4(x_binary)\n",
    "        x_binary = self.dilated_binary_3_5(x_binary)\n",
    "        x_binary = self.asymmetric_binary_3_6(x_binary)\n",
    "        x_binary = self.dilated_binary_3_7(x_binary)\n",
    "\n",
    "        # stage 3 embedding\n",
    "        x_embedding = self.regular_embedding_3_0(x)\n",
    "        x_embedding = self.dilated_embedding_3_1(x_embedding)\n",
    "        x_embedding = self.asymmetric_embedding_3_2(x_embedding)\n",
    "        x_embedding = self.dilated_embedding_3_3(x_embedding)\n",
    "        x_embedding = self.regular_embedding_3_4(x_embedding)\n",
    "        x_embedding = self.dilated_embedding_3_5(x_embedding)\n",
    "        x_embedding = self.asymmetric_bembedding_3_6(x_embedding)\n",
    "        x_embedding = self.dilated_embedding_3_7(x_embedding)\n",
    "\n",
    "        # binary branch\n",
    "        x_binary = self.upsample_binary_4_0(x_binary, max_indices2_0, output_size=stage2_input_size)\n",
    "        x_binary = self.regular_binary_4_1(x_binary)\n",
    "        x_binary = self.regular_binary_4_2(x_binary)\n",
    "        x_binary = self.upsample_binary_5_0(x_binary, max_indices1_0, output_size=stage1_input_size)\n",
    "        x_binary = self.regular_binary_5_1(x_binary)\n",
    "        binary_final_logits = self.binary_transposed_conv(x_binary, output_size=input_size)\n",
    "\n",
    "        # embedding branch\n",
    "        x_embedding = self.upsample_embedding_4_0(x_embedding, max_indices2_0, output_size=stage2_input_size)\n",
    "        x_embedding = self.regular_embedding_4_1(x_embedding)\n",
    "        x_embedding = self.regular_embedding_4_2(x_embedding)\n",
    "        x_embedding = self.upsample_embedding_5_0(x_embedding, max_indices1_0, output_size=stage1_input_size)\n",
    "        x_embedding = self.regular_embedding_5_1(x_embedding)\n",
    "        instance_final_logits = self.embedding_transposed_conv(x_embedding, output_size=input_size)\n",
    "\n",
    "        return binary_final_logits, instance_final_logits\n",
    "    \n",
    "\n",
    "class DiscriminativeLoss(_Loss):\n",
    "    def __init__(self, delta_var=0.5, delta_dist=3,\n",
    "                 norm=2, alpha=1.0, beta=1.0, gamma=0.001,\n",
    "                 device=\"cpu\", reduction=\"mean\", n_clusters=4):\n",
    "        super(DiscriminativeLoss, self).__init__(reduction=reduction)\n",
    "        self.delta_var = delta_var\n",
    "        self.delta_dist = delta_dist\n",
    "        self.norm = norm\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.device = torch.device(device)\n",
    "        self.n_clusters = n_clusters\n",
    "        assert self.norm in [1, 2]\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        assert not target.requires_grad\n",
    "\n",
    "        return self._discriminative_loss(input, target)\n",
    "\n",
    "    def _discriminative_loss(self, input, target):\n",
    "        num_samples=target.size(0)\n",
    "\n",
    "        dis_loss=torch.tensor(0.).to(self.device)\n",
    "        var_loss=torch.tensor(0.).to(self.device)\n",
    "        reg_loss=torch.tensor(0.).to(self.device)\n",
    "        for i in range(num_samples):\n",
    "            clusters=[]\n",
    "            sample_embedding=input[i,:,:,:]\n",
    "            sample_label=target[i,:,:].squeeze()\n",
    "            num_clusters=len(sample_label.unique())-1\n",
    "            vals=sample_label.unique()[1:]\n",
    "            sample_label=sample_label.view(sample_label.size(0)*sample_label.size(1))\n",
    "            sample_embedding=sample_embedding.view(-1,sample_embedding.size(1)*sample_embedding.size(2))\n",
    "            v_loss=torch.tensor(0.).to(self.device)\n",
    "            d_loss=torch.tensor(0.).to(self.device)\n",
    "            r_loss=torch.tensor(0.).to(self.device)\n",
    "            for j in range(num_clusters):\n",
    "                indices=(sample_label==vals[j]).nonzero()\n",
    "                indices=indices.squeeze()\n",
    "                cluster_elements=torch.index_select(sample_embedding,1,indices)\n",
    "                Nc=cluster_elements.size(1)\n",
    "                mean_cluster=cluster_elements.mean(dim=1,keepdim=True)\n",
    "                clusters.append(mean_cluster)\n",
    "                v_loss+=torch.pow((torch.clamp(torch.norm(cluster_elements-mean_cluster)-self.delta_var,min=0.)),2).sum()/Nc\n",
    "                r_loss+=torch.sum(torch.abs(mean_cluster))\n",
    "            for index in range(num_clusters):\n",
    "                for idx,cluster in enumerate(clusters):\n",
    "                    if index==idx:\n",
    "                        continue \n",
    "                    else:\n",
    "                        distance=torch.norm(clusters[index]-cluster)#torch.sqrt(torch.sum(torch.pow(clusters[index]-cluster,2)))\n",
    "                        d_loss+=torch.pow(torch.clamp(self.delta_dist-distance,min=0.),2)\n",
    "            var_loss+=v_loss/num_clusters\n",
    "            dis_loss+=d_loss/(num_clusters*(num_clusters-1))\n",
    "            reg_loss+=r_loss/num_clusters\n",
    "        return self.alpha*(var_loss/num_samples)+self.beta*(dis_loss/num_samples)+self.gamma*(reg_loss/num_samples)\n",
    "\n",
    "  \n",
    "def compute_loss(binary_output, instance_output, binary_label, instance_label):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    binary_loss = ce_loss(binary_output, binary_label)\n",
    "\n",
    "    ds_loss = DiscriminativeLoss(delta_var=0.5, delta_dist=3, alpha=1.0, beta=1.0, gamma=0.001)\n",
    "    instance_loss = ds_loss(instance_output, instance_label)\n",
    "    \n",
    "    return binary_loss, instance_loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25707f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# === Lane Detection Setup ===\n",
    "# Assuming you have ENet class defined somewhere\n",
    "# from lane_detector import ENet\n",
    "\n",
    "# Load the pre-trained lane detection model\n",
    "model_path = r'C:\\Users\\DELL\\Documents\\GitHub\\AutonomousDrivingGanMV\\ENET.pth'\n",
    "enet_model = ENet(2, 4)  # Assuming you used the same model architecture\n",
    "enet_model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "enet_model.eval()\n",
    "\n",
    "# === Object and Traffic Sign Detection Setup ===\n",
    "# Assuming obj_model and sign_model are YOLO models already loaded\n",
    "\n",
    "def process_frame(frame, obj_model, sign_model):\n",
    "    # Make a copy of the original frame for visualization\n",
    "    vis_frame = frame.copy()\n",
    "    \n",
    "    # === Lane Detection ===\n",
    "    # Preprocess for lane detection\n",
    "    input_image = cv2.resize(frame, (512, 256))  # Resize to the model's input size\n",
    "    input_image_gray = cv2.cvtColor(input_image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "    input_image_gray = input_image_gray[..., None]\n",
    "    input_tensor = torch.from_numpy(input_image_gray).float().permute(2, 0, 1)  # Convert to tensor\n",
    "\n",
    "    # Run lane detection\n",
    "    with torch.no_grad():\n",
    "        binary_logits, instance_logits = enet_model(input_tensor.unsqueeze(0))\n",
    "    \n",
    "    # Process lane detection output\n",
    "    binary_seg = torch.argmax(binary_logits, dim=1).squeeze().numpy()\n",
    "    binary_seg_resized = cv2.resize(binary_seg, (frame.shape[1], frame.shape[0]), \n",
    "                                   interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    # Create lane overlay (red)\n",
    "    red_mask = np.zeros_like(frame)\n",
    "    red_mask[binary_seg_resized == 1] = [0, 0, 255]  # Red color for lanes\n",
    "    lane_overlay = cv2.addWeighted(frame, 1, red_mask, 0.7, gamma=0)\n",
    "    \n",
    "    # === Object and Traffic Sign Detection ===\n",
    "    # Run detections\n",
    "    obj_results = obj_model.predict(source=frame, imgsz=640, conf=0.3, verbose=False)[0]\n",
    "    sign_results = sign_model.predict(source=frame, imgsz=640, conf=0.3, verbose=False)[0]\n",
    "\n",
    "    # Draw object detections (green)\n",
    "    for box in obj_results.boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        cls_id = int(box.cls[0])\n",
    "        conf = float(box.conf[0])\n",
    "        label = f\"{obj_results.names[cls_id]} {conf:.2f}\"\n",
    "        cv2.rectangle(lane_overlay, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(lane_overlay, label, (x1, y1 - 10), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Draw traffic sign detections (blue)\n",
    "    for box in sign_results.boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        cls_id = int(box.cls[0])\n",
    "        conf = float(box.conf[0])\n",
    "        label = f\"{sign_results.names[cls_id]} {conf:.2f}\"\n",
    "        cv2.rectangle(lane_overlay, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "        cv2.putText(lane_overlay, label, (x1, y1 - 10), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "    return lane_overlay\n",
    "\n",
    "def process_frames_live_style(obj_model, sign_model, folder_path, frame_delay=10):\n",
    "    image_paths = sorted(Path(folder_path).glob(\"*.png\"))  # Modify if needed\n",
    "    \n",
    "    if not image_paths:\n",
    "        print(\"[ERROR] No frames found in folder.\")\n",
    "        return\n",
    "\n",
    "    for img_path in image_paths:\n",
    "        frame = cv2.imread(str(img_path))\n",
    "        if frame is None:\n",
    "            print(f\"[Warning] Could not read {img_path}\")\n",
    "            continue\n",
    "\n",
    "        # Process the frame with all detectors\n",
    "        processed_frame = process_frame(frame, obj_model, sign_model)\n",
    "\n",
    "        # Show the frame\n",
    "        cv2.imshow(\"Integrated Detection - Press 'q' to Exit\", processed_frame)\n",
    "        if cv2.waitKey(frame_delay) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# === Usage Example ===\n",
    "# Assuming you have obj_model and sign_model loaded\n",
    "frame_folder = r\"C:\\Users\\Sejal Hanmante\\Downloads\\task1train540p\"\n",
    "process_frames_live_style(obj_model, sign_model, frame_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dff64beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.1\n",
      "Keras version: 3.3.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Keras version:\", keras.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938a0496",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
