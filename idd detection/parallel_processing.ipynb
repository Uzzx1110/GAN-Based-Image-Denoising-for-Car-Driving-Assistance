{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fb2955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "from multiprocessing import Process, Queue, set_start_method\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ================= CONFIG =================\n",
    "main_input_dir = r\"D:\\NLPInsuranceProject\\AutonomousDrivingGanMV\\task1test\"\n",
    "output_video_path = \"final_output.mp4\"\n",
    "fps = 30\n",
    "\n",
    "obj_model_path = r\"D:\\NLPInsuranceProject\\AutonomousDrivingGanMV\\idd detection\\shareYolo\\train\\exp12\\weights\\best.pt\"\n",
    "lane_model_path = r\"D:\\NLPInsuranceProject\\AutonomousDrivingGanMV\\idd detection\\models_best\\ENET.pth\"\n",
    "sign_model_path = r\"D:\\NLPInsuranceProject\\AutonomousDrivingGanMV\\idd detection\\models_best\\best.pt\"\n",
    "# ==========================================\n",
    "\n",
    "def load_all_frames(main_dir):\n",
    "    frame_paths = []\n",
    "    for root, _, files in os.walk(main_dir):\n",
    "        image_files = sorted([f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "        for img in image_files:\n",
    "            frame_paths.append(os.path.join(root, img))\n",
    "    return frame_paths\n",
    "\n",
    "def run_yolo_inference(model, input_queue, output_queue, name=\"\"):\n",
    "    while True:\n",
    "        item = input_queue.get()\n",
    "        if item is None:\n",
    "            break\n",
    "        idx, frame = item\n",
    "        results = model.predict(source=frame, imgsz=640, conf=0.3, verbose=False)[0]\n",
    "        annotated = results.plot()\n",
    "        output_queue.put((idx, annotated))\n",
    "\n",
    "def merge_annotations(base_frame, annotated_frames):\n",
    "    merged = base_frame.copy()\n",
    "    for overlay in annotated_frames:\n",
    "        mask = overlay != 0\n",
    "        merged[mask] = overlay[mask]\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a75c461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading YOLO models and ENet lane segmentation model...\n",
      "[INFO] Models loaded and warmed up successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "\n",
    "# ====== CONFIG ======\n",
    "device = 'cpu'  # Change to 'cuda' for GPU acceleration\n",
    "obj_model_path = r\"D:\\NLPInsuranceProject\\AutonomousDrivingGanMV\\idd detection\\shareYolo\\train\\exp12\\weights\\best.pt\"\n",
    "sign_model_path = r\"D:\\NLPInsuranceProject\\AutonomousDrivingGanMV\\idd detection\\models_best\\best.pt\"\n",
    "#lane_model_path = r\"D:\\NLPInsuranceProject\\AutonomousDrivingGanMV\\idd detection\\models_best\\ENET.pth\"\n",
    "# =====================\n",
    "\n",
    "# === Load YOLO models ===\n",
    "def load_yolo_model(model_path):\n",
    "    model = YOLO(model_path)\n",
    "    return model\n",
    "\n",
    "# === Warmup ENet model (Lane Segmentation) ===\n",
    "\"\"\"def load_enet_model(model_path, device):\n",
    "    from models.enet import ENet  # Ensure the ENet model is defined in models/enet.py\n",
    "    model = ENet(12)  # Adjust num_classes if necessary\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    return model\"\"\"\n",
    "\n",
    "# Load models and warmup\n",
    "def warmup_models():\n",
    "    print(\"[INFO] Loading YOLO models and ENet lane segmentation model...\")\n",
    "    obj_model = load_yolo_model(obj_model_path)\n",
    "    sign_model = load_yolo_model(sign_model_path)\n",
    "    #lane_model = load_enet_model(lane_model_path, device)\n",
    "\n",
    "    # Warmup YOLO models with dummy frame\n",
    "    dummy_frame = np.zeros((640, 640, 3), dtype=np.uint8)\n",
    "    for model in [obj_model, sign_model]:\n",
    "        model.predict(source=dummy_frame, imgsz=640, conf=0.3, verbose=False)\n",
    "\n",
    "    print(\"[INFO] Models loaded and warmed up successfully!\")\n",
    "    return obj_model, sign_model    #, lane_model\n",
    "\n",
    "# Call the warmup function to load and prepare models\n",
    "obj_model, sign_model = warmup_models()  #, lane_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9a084cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# === Config ===\n",
    "frame_folder = r\"C:\\Users\\Sejal Hanmante\\Downloads\\task1train540p\"  # Folder with frames\n",
    "frame_delay = 10  # ms delay between frames (adjust for speed)\n",
    "\n",
    "# === Use already-loaded models ===\n",
    "# Assuming obj_model and sign_model are already initialized in your current session.\n",
    "\n",
    "def process_frames_live_style(obj_model, sign_model, folder_path):\n",
    "    image_paths = sorted(Path(folder_path).glob(\"*.png\"))  # Modify if you use .png or .jpeg\n",
    "\n",
    "    if not image_paths:\n",
    "        print(\"[ERROR] No frames found in folder.\")\n",
    "        return\n",
    "\n",
    "    for img_path in image_paths:\n",
    "        frame = cv2.imread(str(img_path))\n",
    "        if frame is None:\n",
    "            print(f\"[Warning] Could not read {img_path}\")\n",
    "            continue\n",
    "\n",
    "        # Perform object detection\n",
    "        obj_results = obj_model.predict(source=frame, imgsz=640, conf=0.3, verbose=False)[0]\n",
    "        sign_results = sign_model.predict(source=frame, imgsz=640, conf=0.3, verbose=False)[0]\n",
    "\n",
    "        # Draw detections from both models\n",
    "        for result, color in zip([obj_results, sign_results], [(0, 255, 0), (0, 0, 255)]):\n",
    "            boxes = result.boxes\n",
    "            if boxes is not None:\n",
    "                for box in boxes:\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                    cls_id = int(box.cls[0])\n",
    "                    conf = float(box.conf[0])\n",
    "                    label = f\"{result.names[cls_id]} {conf:.2f}\"\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                    cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "        # Show the frame in a window\n",
    "        cv2.imshow(\"Live Detection - Press 'q' to Exit\", frame)\n",
    "        if cv2.waitKey(frame_delay) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# === Call the function using existing models ===\n",
    "process_frames_live_style(obj_model, sign_model, frame_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79c6c77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "\n",
    "\n",
    "class LaneDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_path=\"E:/TuSimple/train_set\", train=True, size=(512, 256)):\n",
    "        self._dataset_path = dataset_path\n",
    "        self._mode = \"train\" if train else \"eval\"\n",
    "        self._image_size = size # w, h\n",
    "\n",
    "\n",
    "        if self._mode == \"train\":\n",
    "            label_files = [\n",
    "                os.path.join(self._dataset_path, f\"label_data_{suffix}.json\")\n",
    "                for suffix in (\"0313\", \"0531\")\n",
    "            ]\n",
    "        elif self._mode == \"eval\":\n",
    "            label_files = [\n",
    "                os.path.join(self._dataset_path, f\"label_data_{suffix}.json\")\n",
    "                for suffix in (\"0601\",)\n",
    "            ]\n",
    "\n",
    "        self._data = []\n",
    "\n",
    "        for label_file in label_files:\n",
    "            self._process_label_file(label_file)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self._dataset_path, self._data[idx][0])\n",
    "        image = cv2.imread(image_path)\n",
    "        h, w, c = image.shape\n",
    "        image = cv2.resize(image, self._image_size, interpolation=cv2.INTER_LINEAR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        image = image[..., None]\n",
    "        lanes = self._data[idx][1]\n",
    "\n",
    "        segmentation_image = self._draw(h, w, lanes, \"segmentation\")\n",
    "        instance_image = self._draw(h, w, lanes, \"instance\")\n",
    "\n",
    "        instance_image = instance_image[..., None]\n",
    "\n",
    "        image = torch.from_numpy(image).float().permute((2, 0, 1))\n",
    "        segmentation_image = torch.from_numpy(segmentation_image.copy())\n",
    "        instance_image =  torch.from_numpy(instance_image.copy()).permute((2, 0, 1))\n",
    "        segmentation_image = segmentation_image.to(torch.int64)\n",
    "\n",
    "        return image, segmentation_image, instance_image # 1 x H x W [[0, 1], [2, 0]]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def _draw(self, h, w, lanes, image_type):\n",
    "        image = np.zeros((h, w), dtype=np.uint8)\n",
    "        for i, lane in enumerate(lanes):\n",
    "            color = 1 if image_type == \"segmentation\" else i + 1\n",
    "            cv2.polylines(image, [lane], False, color, 10)\n",
    "\n",
    "        image = cv2.resize(image, self._image_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def _process_label_file(self, file_path):\n",
    "        with open(file_path) as f:\n",
    "            for line in f:\n",
    "                info = json.loads(line)\n",
    "                image = info[\"raw_file\"]\n",
    "                lanes = info[\"lanes\"]\n",
    "                h_samples = info[\"h_samples\"]\n",
    "                lanes_coords = []\n",
    "                for lane in lanes:\n",
    "                    x = np.array([lane]).T\n",
    "                    y = np.array([h_samples]).T\n",
    "                    xy = np.hstack((x, y))\n",
    "                    idx = np.where(xy[:, 0] > 0)\n",
    "                    lane_coords = xy[idx]\n",
    "                    lanes_coords.append(lane_coords)\n",
    "                self._data.append((image, lanes_coords))\n",
    "\n",
    "\n",
    "class InitialBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 bias=False,\n",
    "                 relu=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if relu:\n",
    "            activation = nn.ReLU\n",
    "        else:\n",
    "            activation = nn.PReLU\n",
    "\n",
    "        # Main branch - As stated above the number of output channels for this\n",
    "        # branch is the total minus 3, since the remaining channels come from\n",
    "        # the extension branch\n",
    "        self.main_branch = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels - 1,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            bias=bias)\n",
    "\n",
    "        # Extension branch\n",
    "        self.ext_branch = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "        # Initialize batch normalization to be used after concatenation\n",
    "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # PReLU layer to apply after concatenating the branches\n",
    "        self.out_activation = activation()\n",
    "\n",
    "    def forward(self, x):\n",
    "        main = self.main_branch(x)\n",
    "        ext = self.ext_branch(x)\n",
    "\n",
    "        # Concatenate branches\n",
    "        out = torch.cat((main, ext), 1)\n",
    "\n",
    "        # Apply batch normalization\n",
    "        out = self.batch_norm(out)\n",
    "\n",
    "        return self.out_activation(out)\n",
    "\n",
    "\n",
    "class RegularBottleneck(nn.Module):\n",
    "    def __init__(self,\n",
    "                 channels,\n",
    "                 internal_ratio=4,\n",
    "                 kernel_size=3,\n",
    "                 padding=0,\n",
    "                 dilation=1,\n",
    "                 asymmetric=False,\n",
    "                 dropout_prob=0,\n",
    "                 bias=False,\n",
    "                 relu=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Check in the internal_scale parameter is within the expected range\n",
    "        # [1, channels]\n",
    "        if internal_ratio <= 1 or internal_ratio > channels:\n",
    "            raise RuntimeError(\"Value out of range. Expected value in the \"\n",
    "                               \"interval [1, {0}], got internal_scale={1}.\"\n",
    "                               .format(channels, internal_ratio))\n",
    "\n",
    "        internal_channels = channels // internal_ratio\n",
    "\n",
    "        if relu:\n",
    "            activation = nn.ReLU\n",
    "        else:\n",
    "            activation = nn.PReLU\n",
    "\n",
    "        # Main branch - shortcut connection\n",
    "\n",
    "        # Extension branch - 1x1 convolution, followed by a regular, dilated or\n",
    "        # asymmetric convolution, followed by another 1x1 convolution, and,\n",
    "        # finally, a regularizer (spatial dropout). Number of channels is constant.\n",
    "\n",
    "        # 1x1 projection convolution\n",
    "        self.ext_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                channels,\n",
    "                internal_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                bias=bias), nn.BatchNorm2d(internal_channels), activation())\n",
    "\n",
    "        # If the convolution is asymmetric we split the main convolution in\n",
    "        # two. Eg. for a 5x5 asymmetric convolution we have two convolution:\n",
    "        # the first is 5x1 and the second is 1x5.\n",
    "        if asymmetric:\n",
    "            self.ext_conv2 = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    internal_channels,\n",
    "                    internal_channels,\n",
    "                    kernel_size=(kernel_size, 1),\n",
    "                    stride=1,\n",
    "                    padding=(padding, 0),\n",
    "                    dilation=dilation,\n",
    "                    bias=bias), nn.BatchNorm2d(internal_channels), activation(),\n",
    "                nn.Conv2d(\n",
    "                    internal_channels,\n",
    "                    internal_channels,\n",
    "                    kernel_size=(1, kernel_size),\n",
    "                    stride=1,\n",
    "                    padding=(0, padding),\n",
    "                    dilation=dilation,\n",
    "                    bias=bias), nn.BatchNorm2d(internal_channels), activation())\n",
    "        else:\n",
    "            self.ext_conv2 = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    internal_channels,\n",
    "                    internal_channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=1,\n",
    "                    padding=padding,\n",
    "                    dilation=dilation,\n",
    "                    bias=bias), nn.BatchNorm2d(internal_channels), activation())\n",
    "\n",
    "        # 1x1 expansion convolution\n",
    "        self.ext_conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                internal_channels,\n",
    "                channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                bias=bias), nn.BatchNorm2d(channels), activation())\n",
    "\n",
    "        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n",
    "\n",
    "        # PReLU layer to apply after adding the branches\n",
    "        self.out_activation = activation()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Main branch shortcut\n",
    "        main = x\n",
    "\n",
    "        # Extension branch\n",
    "        ext = self.ext_conv1(x)\n",
    "        ext = self.ext_conv2(ext)\n",
    "        ext = self.ext_conv3(ext)\n",
    "        ext = self.ext_regul(ext)\n",
    "\n",
    "        # Add main and extension branches\n",
    "        out = main + ext\n",
    "\n",
    "        return self.out_activation(out)\n",
    "\n",
    "\n",
    "class DownsamplingBottleneck(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 internal_ratio=4,\n",
    "                 return_indices=False,\n",
    "                 dropout_prob=0,\n",
    "                 bias=False,\n",
    "                 relu=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Store parameters that are needed later\n",
    "        self.return_indices = return_indices\n",
    "\n",
    "        # Check in the internal_scale parameter is within the expected range\n",
    "        # [1, channels]\n",
    "        if internal_ratio <= 1 or internal_ratio > in_channels:\n",
    "            raise RuntimeError(\"Value out of range. Expected value in the \"\n",
    "                               \"interval [1, {0}], got internal_scale={1}. \"\n",
    "                               .format(in_channels, internal_ratio))\n",
    "\n",
    "        internal_channels = in_channels // internal_ratio\n",
    "\n",
    "        if relu:\n",
    "            activation = nn.ReLU\n",
    "        else:\n",
    "            activation = nn.PReLU\n",
    "\n",
    "        # Main branch - max pooling followed by feature map (channels) padding\n",
    "        self.main_max1 = nn.MaxPool2d(\n",
    "            2,\n",
    "            stride=2,\n",
    "            return_indices=return_indices)\n",
    "\n",
    "        # Extension branch - 2x2 convolution, followed by a regular, dilated or\n",
    "        # asymmetric convolution, followed by another 1x1 convolution. Number\n",
    "        # of channels is doubled.\n",
    "\n",
    "        # 2x2 projection convolution with stride 2\n",
    "        self.ext_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                internal_channels,\n",
    "                kernel_size=2,\n",
    "                stride=2,\n",
    "                bias=bias), nn.BatchNorm2d(internal_channels), activation())\n",
    "\n",
    "        # Convolution\n",
    "        self.ext_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                internal_channels,\n",
    "                internal_channels,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=bias), nn.BatchNorm2d(internal_channels), activation())\n",
    "\n",
    "        # 1x1 expansion convolution\n",
    "        self.ext_conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                internal_channels,\n",
    "                out_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                bias=bias), nn.BatchNorm2d(out_channels), activation())\n",
    "\n",
    "        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n",
    "\n",
    "        # PReLU layer to apply after concatenating the branches\n",
    "        self.out_activation = activation()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Main branch shortcut\n",
    "        if self.return_indices:\n",
    "            main, max_indices = self.main_max1(x)\n",
    "        else:\n",
    "            main = self.main_max1(x)\n",
    "\n",
    "        # Extension branch\n",
    "        ext = self.ext_conv1(x)\n",
    "        ext = self.ext_conv2(ext)\n",
    "        ext = self.ext_conv3(ext)\n",
    "        ext = self.ext_regul(ext)\n",
    "\n",
    "        # Main branch channel padding\n",
    "        n, ch_ext, h, w = ext.size()\n",
    "        ch_main = main.size()[1]\n",
    "        padding = torch.zeros(n, ch_ext - ch_main, h, w)\n",
    "\n",
    "        # Before concatenating, check if main is on the CPU or GPU and\n",
    "        # convert padding accordingly\n",
    "        if main.is_cuda:\n",
    "            padding = padding.cuda()\n",
    "\n",
    "        # Concatenate\n",
    "        main = torch.cat((main, padding), 1)\n",
    "\n",
    "        # Add main and extension branches\n",
    "        out = main + ext\n",
    "\n",
    "        return self.out_activation(out), max_indices\n",
    "\n",
    "\n",
    "class UpsamplingBottleneck(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 internal_ratio=4,\n",
    "                 dropout_prob=0,\n",
    "                 bias=False,\n",
    "                 relu=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Check in the internal_scale parameter is within the expected range\n",
    "        # [1, channels]\n",
    "        if internal_ratio <= 1 or internal_ratio > in_channels:\n",
    "            raise RuntimeError(\"Value out of range. Expected value in the \"\n",
    "                               \"interval [1, {0}], got internal_scale={1}. \"\n",
    "                               .format(in_channels, internal_ratio))\n",
    "\n",
    "        internal_channels = in_channels // internal_ratio\n",
    "\n",
    "        if relu:\n",
    "            activation = nn.ReLU\n",
    "        else:\n",
    "            activation = nn.PReLU\n",
    "\n",
    "        # Main branch - max pooling followed by feature map (channels) padding\n",
    "        self.main_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias),\n",
    "            nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        # Remember that the stride is the same as the kernel_size, just like\n",
    "        # the max pooling layers\n",
    "        self.main_unpool1 = nn.MaxUnpool2d(kernel_size=2)\n",
    "\n",
    "        # Extension branch - 1x1 convolution, followed by a regular, dilated or\n",
    "        # asymmetric convolution, followed by another 1x1 convolution. Number\n",
    "        # of channels is doubled.\n",
    "\n",
    "        # 1x1 projection convolution with stride 1\n",
    "        self.ext_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels, internal_channels, kernel_size=1, bias=bias),\n",
    "            nn.BatchNorm2d(internal_channels), activation())\n",
    "\n",
    "        # Transposed convolution\n",
    "        self.ext_tconv1 = nn.ConvTranspose2d(\n",
    "            internal_channels,\n",
    "            internal_channels,\n",
    "            kernel_size=2,\n",
    "            stride=2,\n",
    "            bias=bias)\n",
    "        self.ext_tconv1_bnorm = nn.BatchNorm2d(internal_channels)\n",
    "        self.ext_tconv1_activation = activation()\n",
    "\n",
    "        # 1x1 expansion convolution\n",
    "        self.ext_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                internal_channels, out_channels, kernel_size=1, bias=bias),\n",
    "            nn.BatchNorm2d(out_channels), activation())\n",
    "\n",
    "        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n",
    "\n",
    "        # PReLU layer to apply after concatenating the branches\n",
    "        self.out_activation = activation()\n",
    "\n",
    "    def forward(self, x, max_indices, output_size):\n",
    "        # Main branch shortcut\n",
    "        main = self.main_conv1(x)\n",
    "        main = self.main_unpool1(\n",
    "            main, max_indices, output_size=output_size)\n",
    "\n",
    "        # Extension branch\n",
    "        ext = self.ext_conv1(x)\n",
    "        ext = self.ext_tconv1(ext, output_size=output_size)\n",
    "        ext = self.ext_tconv1_bnorm(ext)\n",
    "        ext = self.ext_tconv1_activation(ext)\n",
    "        ext = self.ext_conv2(ext)\n",
    "        ext = self.ext_regul(ext)\n",
    "\n",
    "        # Add main and extension branches\n",
    "        out = main + ext\n",
    "\n",
    "        return self.out_activation(out)\n",
    "\n",
    "\n",
    "class ENet(nn.Module):\n",
    "    def __init__(self, binary_seg, embedding_dim, encoder_relu=False, decoder_relu=True):\n",
    "        super(ENet, self).__init__()\n",
    "\n",
    "        self.initial_block = InitialBlock(1, 16, relu=encoder_relu)\n",
    "\n",
    "        # Stage 1 share\n",
    "        self.downsample1_0 = DownsamplingBottleneck(16, 64, return_indices=True, dropout_prob=0.01, relu=encoder_relu)\n",
    "        self.regular1_1 = RegularBottleneck(64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n",
    "        self.regular1_2 = RegularBottleneck(64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n",
    "        self.regular1_3 = RegularBottleneck(64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n",
    "        self.regular1_4 = RegularBottleneck(64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n",
    "\n",
    "        # Stage 2 share\n",
    "        self.downsample2_0 = DownsamplingBottleneck(64, 128, return_indices=True, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.regular2_1 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.dilated2_2 = RegularBottleneck(128, dilation=2, padding=2, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.asymmetric2_3 = RegularBottleneck(128, kernel_size=5, padding=2, asymmetric=True, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.dilated2_4 = RegularBottleneck(128, dilation=4, padding=4, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.regular2_5 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.dilated2_6 = RegularBottleneck(128, dilation=8, padding=8, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.asymmetric2_7 = RegularBottleneck(128, kernel_size=5, asymmetric=True, padding=2, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.dilated2_8 = RegularBottleneck(128, dilation=16, padding=16, dropout_prob=0.1, relu=encoder_relu)\n",
    "\n",
    "        # stage 3 binary\n",
    "        self.regular_binary_3_0 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.dilated_binary_3_1 = RegularBottleneck(128, dilation=2, padding=2, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.asymmetric_binary_3_2 = RegularBottleneck(128, kernel_size=5, padding=2, asymmetric=True, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.dilated_binary_3_3 = RegularBottleneck(128, dilation=4, padding=4, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.regular_binary_3_4 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.dilated_binary_3_5 = RegularBottleneck(128, dilation=8, padding=8, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.asymmetric_binary_3_6 = RegularBottleneck(128, kernel_size=5, asymmetric=True, padding=2, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.dilated_binary_3_7 = RegularBottleneck(128, dilation=16, padding=16, dropout_prob=0.1, relu=encoder_relu)\n",
    "\n",
    "        # stage 3 embedding\n",
    "        self.regular_embedding_3_0 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.dilated_embedding_3_1 = RegularBottleneck(128, dilation=2, padding=2, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.asymmetric_embedding_3_2 = RegularBottleneck(128, kernel_size=5, padding=2, asymmetric=True, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.dilated_embedding_3_3 = RegularBottleneck(128, dilation=4, padding=4, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.regular_embedding_3_4 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.dilated_embedding_3_5 = RegularBottleneck(128, dilation=8, padding=8, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.asymmetric_bembedding_3_6 = RegularBottleneck(128, kernel_size=5, asymmetric=True, padding=2, dropout_prob=0.1, relu=encoder_relu)\n",
    "        self.dilated_embedding_3_7 = RegularBottleneck(128, dilation=16, padding=16, dropout_prob=0.1, relu=encoder_relu)\n",
    "\n",
    "        # binary branch\n",
    "        self.upsample_binary_4_0 = UpsamplingBottleneck(128, 64, dropout_prob=0.1, relu=decoder_relu)\n",
    "        self.regular_binary_4_1 = RegularBottleneck(64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n",
    "        self.regular_binary_4_2 = RegularBottleneck(64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n",
    "        self.upsample_binary_5_0 = UpsamplingBottleneck(64, 16, dropout_prob=0.1, relu=decoder_relu)\n",
    "        self.regular_binary_5_1 = RegularBottleneck(16, padding=1, dropout_prob=0.1, relu=decoder_relu)\n",
    "        self.binary_transposed_conv = nn.ConvTranspose2d(16, binary_seg, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "\n",
    "        # embedding branch\n",
    "        self.upsample_embedding_4_0 = UpsamplingBottleneck(128, 64, dropout_prob=0.1, relu=decoder_relu)\n",
    "        self.regular_embedding_4_1 = RegularBottleneck(64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n",
    "        self.regular_embedding_4_2 = RegularBottleneck(64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n",
    "        self.upsample_embedding_5_0 = UpsamplingBottleneck(64, 16, dropout_prob=0.1, relu=decoder_relu)\n",
    "        self.regular_embedding_5_1 = RegularBottleneck(16, padding=1, dropout_prob=0.1, relu=decoder_relu)\n",
    "        self.embedding_transposed_conv = nn.ConvTranspose2d(16, embedding_dim, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial block\n",
    "        input_size = x.size()\n",
    "        x = self.initial_block(x)\n",
    "\n",
    "        # Stage 1 share\n",
    "        stage1_input_size = x.size()\n",
    "        x, max_indices1_0 = self.downsample1_0(x)\n",
    "        x = self.regular1_1(x)\n",
    "        x = self.regular1_2(x)\n",
    "        x = self.regular1_3(x)\n",
    "        x = self.regular1_4(x)\n",
    "\n",
    "        # Stage 2 share\n",
    "        stage2_input_size = x.size()\n",
    "        x, max_indices2_0 = self.downsample2_0(x)\n",
    "        x = self.regular2_1(x)\n",
    "        x = self.dilated2_2(x)\n",
    "        x = self.asymmetric2_3(x)\n",
    "        x = self.dilated2_4(x)\n",
    "        x = self.regular2_5(x)\n",
    "        x = self.dilated2_6(x)\n",
    "        x = self.asymmetric2_7(x)\n",
    "        x = self.dilated2_8(x)\n",
    "\n",
    "        # stage 3 binary\n",
    "        x_binary = self.regular_binary_3_0(x)\n",
    "        x_binary = self.dilated_binary_3_1(x_binary)\n",
    "        x_binary = self.asymmetric_binary_3_2(x_binary)\n",
    "        x_binary = self.dilated_binary_3_3(x_binary)\n",
    "        x_binary = self.regular_binary_3_4(x_binary)\n",
    "        x_binary = self.dilated_binary_3_5(x_binary)\n",
    "        x_binary = self.asymmetric_binary_3_6(x_binary)\n",
    "        x_binary = self.dilated_binary_3_7(x_binary)\n",
    "\n",
    "        # stage 3 embedding\n",
    "        x_embedding = self.regular_embedding_3_0(x)\n",
    "        x_embedding = self.dilated_embedding_3_1(x_embedding)\n",
    "        x_embedding = self.asymmetric_embedding_3_2(x_embedding)\n",
    "        x_embedding = self.dilated_embedding_3_3(x_embedding)\n",
    "        x_embedding = self.regular_embedding_3_4(x_embedding)\n",
    "        x_embedding = self.dilated_embedding_3_5(x_embedding)\n",
    "        x_embedding = self.asymmetric_bembedding_3_6(x_embedding)\n",
    "        x_embedding = self.dilated_embedding_3_7(x_embedding)\n",
    "\n",
    "        # binary branch\n",
    "        x_binary = self.upsample_binary_4_0(x_binary, max_indices2_0, output_size=stage2_input_size)\n",
    "        x_binary = self.regular_binary_4_1(x_binary)\n",
    "        x_binary = self.regular_binary_4_2(x_binary)\n",
    "        x_binary = self.upsample_binary_5_0(x_binary, max_indices1_0, output_size=stage1_input_size)\n",
    "        x_binary = self.regular_binary_5_1(x_binary)\n",
    "        binary_final_logits = self.binary_transposed_conv(x_binary, output_size=input_size)\n",
    "\n",
    "        # embedding branch\n",
    "        x_embedding = self.upsample_embedding_4_0(x_embedding, max_indices2_0, output_size=stage2_input_size)\n",
    "        x_embedding = self.regular_embedding_4_1(x_embedding)\n",
    "        x_embedding = self.regular_embedding_4_2(x_embedding)\n",
    "        x_embedding = self.upsample_embedding_5_0(x_embedding, max_indices1_0, output_size=stage1_input_size)\n",
    "        x_embedding = self.regular_embedding_5_1(x_embedding)\n",
    "        instance_final_logits = self.embedding_transposed_conv(x_embedding, output_size=input_size)\n",
    "\n",
    "        return binary_final_logits, instance_final_logits\n",
    "    \n",
    "\n",
    "class DiscriminativeLoss(_Loss):\n",
    "    def __init__(self, delta_var=0.5, delta_dist=3,\n",
    "                 norm=2, alpha=1.0, beta=1.0, gamma=0.001,\n",
    "                 device=\"cpu\", reduction=\"mean\", n_clusters=4):\n",
    "        super(DiscriminativeLoss, self).__init__(reduction=reduction)\n",
    "        self.delta_var = delta_var\n",
    "        self.delta_dist = delta_dist\n",
    "        self.norm = norm\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.device = torch.device(device)\n",
    "        self.n_clusters = n_clusters\n",
    "        assert self.norm in [1, 2]\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        assert not target.requires_grad\n",
    "\n",
    "        return self._discriminative_loss(input, target)\n",
    "\n",
    "    def _discriminative_loss(self, input, target):\n",
    "        num_samples=target.size(0)\n",
    "\n",
    "        dis_loss=torch.tensor(0.).to(self.device)\n",
    "        var_loss=torch.tensor(0.).to(self.device)\n",
    "        reg_loss=torch.tensor(0.).to(self.device)\n",
    "        for i in range(num_samples):\n",
    "            clusters=[]\n",
    "            sample_embedding=input[i,:,:,:]\n",
    "            sample_label=target[i,:,:].squeeze()\n",
    "            num_clusters=len(sample_label.unique())-1\n",
    "            vals=sample_label.unique()[1:]\n",
    "            sample_label=sample_label.view(sample_label.size(0)*sample_label.size(1))\n",
    "            sample_embedding=sample_embedding.view(-1,sample_embedding.size(1)*sample_embedding.size(2))\n",
    "            v_loss=torch.tensor(0.).to(self.device)\n",
    "            d_loss=torch.tensor(0.).to(self.device)\n",
    "            r_loss=torch.tensor(0.).to(self.device)\n",
    "            for j in range(num_clusters):\n",
    "                indices=(sample_label==vals[j]).nonzero()\n",
    "                indices=indices.squeeze()\n",
    "                cluster_elements=torch.index_select(sample_embedding,1,indices)\n",
    "                Nc=cluster_elements.size(1)\n",
    "                mean_cluster=cluster_elements.mean(dim=1,keepdim=True)\n",
    "                clusters.append(mean_cluster)\n",
    "                v_loss+=torch.pow((torch.clamp(torch.norm(cluster_elements-mean_cluster)-self.delta_var,min=0.)),2).sum()/Nc\n",
    "                r_loss+=torch.sum(torch.abs(mean_cluster))\n",
    "            for index in range(num_clusters):\n",
    "                for idx,cluster in enumerate(clusters):\n",
    "                    if index==idx:\n",
    "                        continue \n",
    "                    else:\n",
    "                        distance=torch.norm(clusters[index]-cluster)#torch.sqrt(torch.sum(torch.pow(clusters[index]-cluster,2)))\n",
    "                        d_loss+=torch.pow(torch.clamp(self.delta_dist-distance,min=0.),2)\n",
    "            var_loss+=v_loss/num_clusters\n",
    "            dis_loss+=d_loss/(num_clusters*(num_clusters-1))\n",
    "            reg_loss+=r_loss/num_clusters\n",
    "        return self.alpha*(var_loss/num_samples)+self.beta*(dis_loss/num_samples)+self.gamma*(reg_loss/num_samples)\n",
    "\n",
    "  \n",
    "def compute_loss(binary_output, instance_output, binary_label, instance_label):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    binary_loss = ce_loss(binary_output, binary_label)\n",
    "\n",
    "    ds_loss = DiscriminativeLoss(delta_var=0.5, delta_dist=3, alpha=1.0, beta=1.0, gamma=0.001)\n",
    "    instance_loss = ds_loss(instance_output, instance_label)\n",
    "    \n",
    "    return binary_loss, instance_loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25707f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# === Lane Detection Setup ===\n",
    "# Assuming you have ENet class defined somewhere\n",
    "# from lane_detector import ENet\n",
    "\n",
    "# Load the pre-trained lane detection model\n",
    "model_path = r'D:\\NLPInsuranceProject\\AutonomousDrivingGanMV\\idd detection\\models_best\\ENET.pth'\n",
    "enet_model = ENet(2, 4)  # Assuming you used the same model architecture\n",
    "enet_model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "enet_model.eval()\n",
    "\n",
    "# === Object and Traffic Sign Detection Setup ===\n",
    "# Assuming obj_model and sign_model are YOLO models already loaded\n",
    "\n",
    "def process_frame(frame, obj_model, sign_model):\n",
    "    # Make a copy of the original frame for visualization\n",
    "    vis_frame = frame.copy()\n",
    "    \n",
    "    # === Lane Detection ===\n",
    "    # Preprocess for lane detection\n",
    "    input_image = cv2.resize(frame, (512, 256))  # Resize to the model's input size\n",
    "    input_image_gray = cv2.cvtColor(input_image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "    input_image_gray = input_image_gray[..., None]\n",
    "    input_tensor = torch.from_numpy(input_image_gray).float().permute(2, 0, 1)  # Convert to tensor\n",
    "\n",
    "    # Run lane detection\n",
    "    with torch.no_grad():\n",
    "        binary_logits, instance_logits = enet_model(input_tensor.unsqueeze(0))\n",
    "    \n",
    "    # Process lane detection output\n",
    "    binary_seg = torch.argmax(binary_logits, dim=1).squeeze().numpy()\n",
    "    binary_seg_resized = cv2.resize(binary_seg, (frame.shape[1], frame.shape[0]), \n",
    "                                   interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    # Create lane overlay (red)\n",
    "    red_mask = np.zeros_like(frame)\n",
    "    red_mask[binary_seg_resized == 1] = [0, 0, 255]  # Red color for lanes\n",
    "    lane_overlay = cv2.addWeighted(frame, 1, red_mask, 0.7, gamma=0)\n",
    "    \n",
    "    # === Object and Traffic Sign Detection ===\n",
    "    # Run detections\n",
    "    obj_results = obj_model.predict(source=frame, imgsz=640, conf=0.3, verbose=False)[0]\n",
    "    sign_results = sign_model.predict(source=frame, imgsz=640, conf=0.3, verbose=False)[0]\n",
    "\n",
    "    # Draw object detections (green)\n",
    "    for box in obj_results.boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        cls_id = int(box.cls[0])\n",
    "        conf = float(box.conf[0])\n",
    "        label = f\"{obj_results.names[cls_id]} {conf:.2f}\"\n",
    "        cv2.rectangle(lane_overlay, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(lane_overlay, label, (x1, y1 - 10), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Draw traffic sign detections (blue)\n",
    "    for box in sign_results.boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        cls_id = int(box.cls[0])\n",
    "        conf = float(box.conf[0])\n",
    "        label = f\"{sign_results.names[cls_id]} {conf:.2f}\"\n",
    "        cv2.rectangle(lane_overlay, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "        cv2.putText(lane_overlay, label, (x1, y1 - 10), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "    return lane_overlay\n",
    "\n",
    "def process_frames_live_style(obj_model, sign_model, folder_path, frame_delay=10):\n",
    "    image_paths = sorted(Path(folder_path).glob(\"*.png\"))  # Modify if needed\n",
    "    \n",
    "    if not image_paths:\n",
    "        print(\"[ERROR] No frames found in folder.\")\n",
    "        return\n",
    "\n",
    "    for img_path in image_paths:\n",
    "        frame = cv2.imread(str(img_path))\n",
    "        if frame is None:\n",
    "            print(f\"[Warning] Could not read {img_path}\")\n",
    "            continue\n",
    "\n",
    "        # Process the frame with all detectors\n",
    "        processed_frame = process_frame(frame, obj_model, sign_model)\n",
    "\n",
    "        # Show the frame\n",
    "        cv2.imshow(\"Integrated Detection - Press 'q' to Exit\", processed_frame)\n",
    "        if cv2.waitKey(frame_delay) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# === Usage Example ===\n",
    "# Assuming you have obj_model and sign_model loaded\n",
    "frame_folder = r\"C:\\Users\\Sejal Hanmante\\Downloads\\task1train540p\"\n",
    "process_frames_live_style(obj_model, sign_model, frame_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e3aebe",
   "metadata": {},
   "source": [
    "## distance calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dff64beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Sejal Hanmante/.cache\\torch\\hub\\intel-isl_MiDaS_master\n",
      "c:\\Users\\Sejal Hanmante\\anaconda3\\envs\\idd_detector\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Sejal Hanmante\\anaconda3\\envs\\idd_detector\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(yolo_model_path)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Load MiDaS depth estimation model from Torch Hub\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m midas \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mintel-isl/MiDaS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDPT_Large\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# You can use \"MiDaS_small\" for speed\u001b[39;00m\n\u001b[0;32m     15\u001b[0m midas_transforms \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintel-isl/MiDaS\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransforms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdpt_transform\n\u001b[0;32m     17\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Sejal Hanmante\\anaconda3\\envs\\idd_detector\\lib\\site-packages\\torch\\hub.py:647\u001b[0m, in \u001b[0;36mload\u001b[1;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    638\u001b[0m     repo_or_dir \u001b[38;5;241m=\u001b[39m _get_cache_or_reload(\n\u001b[0;32m    639\u001b[0m         repo_or_dir,\n\u001b[0;32m    640\u001b[0m         force_reload,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    644\u001b[0m         skip_validation\u001b[38;5;241m=\u001b[39mskip_validation,\n\u001b[0;32m    645\u001b[0m     )\n\u001b[1;32m--> 647\u001b[0m model \u001b[38;5;241m=\u001b[39m _load_local(repo_or_dir, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\Sejal Hanmante\\anaconda3\\envs\\idd_detector\\lib\\site-packages\\torch\\hub.py:676\u001b[0m, in \u001b[0;36m_load_local\u001b[1;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    673\u001b[0m     hub_module \u001b[38;5;241m=\u001b[39m _import_module(MODULE_HUBCONF, hubconf_path)\n\u001b[0;32m    675\u001b[0m     entry \u001b[38;5;241m=\u001b[39m _load_entry_from_hubconf(hub_module, model)\n\u001b[1;32m--> 676\u001b[0m     model \u001b[38;5;241m=\u001b[39m entry(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\intel-isl_MiDaS_master\\hubconf.py:224\u001b[0m, in \u001b[0;36mDPT_Large\u001b[1;34m(pretrained, **kwargs)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mDPT_Large\u001b[39m(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    219\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" # This docstring shows up in hub.help()\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03m    MiDaS DPT-Large model for monocular depth estimation\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;124;03m    pretrained (bool): load pretrained weights into model\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 224\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mDPTDepthModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbackbone\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvitl16_384\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnon_negative\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pretrained:\n\u001b[0;32m    231\u001b[0m         checkpoint \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    232\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/isl-org/MiDaS/releases/download/v3/dpt_large_384.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m         )\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\intel-isl_MiDaS_master\\midas\\dpt_depth.py:160\u001b[0m, in \u001b[0;36mDPTDepthModel.__init__\u001b[1;34m(self, path, non_negative, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhead_features_2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    150\u001b[0m head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m    151\u001b[0m     nn\u001b[38;5;241m.\u001b[39mConv2d(head_features_1, head_features_1 \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    152\u001b[0m     Interpolate(scale_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m     nn\u001b[38;5;241m.\u001b[39mIdentity(),\n\u001b[0;32m    158\u001b[0m )\n\u001b[1;32m--> 160\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(head, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m    \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload(path)\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\intel-isl_MiDaS_master\\midas\\dpt_depth.py:71\u001b[0m, in \u001b[0;36mDPT.__init__\u001b[1;34m(self, head, features, backbone, readout, channels_last, use_bn, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m     in_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Instantiate backbone and reassemble blocks\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrained, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscratch \u001b[38;5;241m=\u001b[39m \u001b[43m_make_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Set to true of you want to train from scratch, uses ImageNet weights\u001b[39;49;00m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexportable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_readout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreadout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumber_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(hooks) \u001b[38;5;28;01mif\u001b[39;00m hooks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m     84\u001b[0m size_refinenet3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\intel-isl_MiDaS_master\\midas\\blocks.py:97\u001b[0m, in \u001b[0;36m_make_encoder\u001b[1;34m(backbone, features, use_pretrained, groups, expand, exportable, hooks, use_vit_only, use_readout, in_features)\u001b[0m\n\u001b[0;32m     93\u001b[0m     scratch \u001b[38;5;241m=\u001b[39m _make_scratch(\n\u001b[0;32m     94\u001b[0m         [\u001b[38;5;241m384\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m768\u001b[39m], features, groups\u001b[38;5;241m=\u001b[39mgroups, expand\u001b[38;5;241m=\u001b[39mexpand\n\u001b[0;32m     95\u001b[0m     )  \u001b[38;5;66;03m# LeViT 384 (backbone)\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backbone \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvitl16_384\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 97\u001b[0m     pretrained \u001b[38;5;241m=\u001b[39m \u001b[43m_make_pretrained_vitl16_384\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_pretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_readout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_readout\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m     scratch \u001b[38;5;241m=\u001b[39m _make_scratch(\n\u001b[0;32m    101\u001b[0m         [\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m], features, groups\u001b[38;5;241m=\u001b[39mgroups, expand\u001b[38;5;241m=\u001b[39mexpand\n\u001b[0;32m    102\u001b[0m     )  \u001b[38;5;66;03m# ViT-L/16 - 85.0% Top1 (backbone)\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backbone \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvitb_rn50_384\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\intel-isl_MiDaS_master\\midas\\backbones\\vit.py:99\u001b[0m, in \u001b[0;36m_make_pretrained_vitl16_384\u001b[1;34m(pretrained, use_readout, hooks)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_make_pretrained_vitl16_384\u001b[39m(pretrained, use_readout\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, hooks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 99\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtimm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvit_large_patch16_384\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m     hooks \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m17\u001b[39m, \u001b[38;5;241m23\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m hooks \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m hooks\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _make_vit_b16_backbone(\n\u001b[0;32m    103\u001b[0m         model,\n\u001b[0;32m    104\u001b[0m         features\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    107\u001b[0m         use_readout\u001b[38;5;241m=\u001b[39muse_readout,\n\u001b[0;32m    108\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Sejal Hanmante\\anaconda3\\envs\\idd_detector\\lib\\site-packages\\timm\\models\\_factory.py:126\u001b[0m, in \u001b[0;36mcreate_model\u001b[1;34m(model_name, pretrained, pretrained_cfg, pretrained_cfg_overlay, checkpoint_path, cache_dir, scriptable, exportable, no_jit, **kwargs)\u001b[0m\n\u001b[0;32m    124\u001b[0m create_fn \u001b[38;5;241m=\u001b[39m model_entrypoint(model_name)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_layer_config(scriptable\u001b[38;5;241m=\u001b[39mscriptable, exportable\u001b[38;5;241m=\u001b[39mexportable, no_jit\u001b[38;5;241m=\u001b[39mno_jit):\n\u001b[1;32m--> 126\u001b[0m     model \u001b[38;5;241m=\u001b[39m create_fn(\n\u001b[0;32m    127\u001b[0m         pretrained\u001b[38;5;241m=\u001b[39mpretrained,\n\u001b[0;32m    128\u001b[0m         pretrained_cfg\u001b[38;5;241m=\u001b[39mpretrained_cfg,\n\u001b[0;32m    129\u001b[0m         pretrained_cfg_overlay\u001b[38;5;241m=\u001b[39mpretrained_cfg_overlay,\n\u001b[0;32m    130\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    131\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    132\u001b[0m     )\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_path:\n\u001b[0;32m    135\u001b[0m     load_checkpoint(model, checkpoint_path)\n",
      "File \u001b[1;32mc:\\Users\\Sejal Hanmante\\anaconda3\\envs\\idd_detector\\lib\\site-packages\\timm\\models\\vision_transformer.py:2565\u001b[0m, in \u001b[0;36mvit_large_patch16_384\u001b[1;34m(pretrained, **kwargs)\u001b[0m\n\u001b[0;32m   2561\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" ViT-Large model (ViT-L/16) from original paper (https://arxiv.org/abs/2010.11929).\u001b[39;00m\n\u001b[0;32m   2562\u001b[0m \u001b[38;5;124;03mImageNet-1k weights fine-tuned from in21k @ 384x384, source https://github.com/google-research/vision_transformer.\u001b[39;00m\n\u001b[0;32m   2563\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2564\u001b[0m model_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m-> 2565\u001b[0m model \u001b[38;5;241m=\u001b[39m _create_vision_transformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvit_large_patch16_384\u001b[39m\u001b[38;5;124m'\u001b[39m, pretrained\u001b[38;5;241m=\u001b[39mpretrained, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(model_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m   2566\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\Sejal Hanmante\\anaconda3\\envs\\idd_detector\\lib\\site-packages\\timm\\models\\vision_transformer.py:2406\u001b[0m, in \u001b[0;36m_create_vision_transformer\u001b[1;34m(variant, pretrained, **kwargs)\u001b[0m\n\u001b[0;32m   2403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msiglip\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m variant \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal_pool\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   2404\u001b[0m     strict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 2406\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m build_model_with_cfg(\n\u001b[0;32m   2407\u001b[0m     VisionTransformer,\n\u001b[0;32m   2408\u001b[0m     variant,\n\u001b[0;32m   2409\u001b[0m     pretrained,\n\u001b[0;32m   2410\u001b[0m     pretrained_filter_fn\u001b[38;5;241m=\u001b[39m_filter_fn,\n\u001b[0;32m   2411\u001b[0m     pretrained_strict\u001b[38;5;241m=\u001b[39mstrict,\n\u001b[0;32m   2412\u001b[0m     feature_cfg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(out_indices\u001b[38;5;241m=\u001b[39mout_indices, feature_cls\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgetter\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m   2413\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2414\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Sejal Hanmante\\anaconda3\\envs\\idd_detector\\lib\\site-packages\\timm\\models\\_builder.py:424\u001b[0m, in \u001b[0;36mbuild_model_with_cfg\u001b[1;34m(model_cls, variant, pretrained, pretrained_cfg, pretrained_cfg_overlay, model_cfg, feature_cfg, pretrained_strict, pretrained_filter_fn, cache_dir, kwargs_filter, **kwargs)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;66;03m# Instantiate the model\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 424\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_cls(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    426\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_cls(cfg\u001b[38;5;241m=\u001b[39mmodel_cfg, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Sejal Hanmante\\anaconda3\\envs\\idd_detector\\lib\\site-packages\\timm\\models\\vision_transformer.py:596\u001b[0m, in \u001b[0;36mVisionTransformer.__init__\u001b[1;34m(self, img_size, patch_size, in_chans, num_classes, global_pool, embed_dim, depth, num_heads, mlp_ratio, qkv_bias, qk_norm, proj_bias, init_values, class_token, pos_embed, no_embed_class, reg_tokens, pre_norm, final_norm, fc_norm, dynamic_img_size, dynamic_img_pad, drop_rate, pos_drop_rate, patch_drop_rate, proj_drop_rate, attn_drop_rate, drop_path_rate, weight_init, fix_init, embed_layer, embed_norm_layer, norm_layer, act_layer, block_fn, mlp_layer)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, num_classes) \u001b[38;5;28;01mif\u001b[39;00m num_classes \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mIdentity()\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_init \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 596\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_init\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fix_init:\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfix_init_weight()\n",
      "File \u001b[1;32mc:\\Users\\Sejal Hanmante\\anaconda3\\envs\\idd_detector\\lib\\site-packages\\timm\\models\\vision_transformer.py:617\u001b[0m, in \u001b[0;36mVisionTransformer.init_weights\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    616\u001b[0m     nn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mnormal_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_token, std\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n\u001b[1;32m--> 617\u001b[0m \u001b[43mnamed_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_init_weights_vit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_bias\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sejal Hanmante\\anaconda3\\envs\\idd_detector\\lib\\site-packages\\timm\\models\\_manipulate.py:38\u001b[0m, in \u001b[0;36mnamed_apply\u001b[1;34m(fn, module, name, depth_first, include_root)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child_name, child_module \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[0;32m     37\u001b[0m     child_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin((name, child_name)) \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;28;01melse\u001b[39;00m child_name\n\u001b[1;32m---> 38\u001b[0m     \u001b[43mnamed_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchild_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchild_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdepth_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m depth_first \u001b[38;5;129;01mand\u001b[39;00m include_root:\n\u001b[0;32m     40\u001b[0m     fn(module\u001b[38;5;241m=\u001b[39mmodule, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32mc:\\Users\\Sejal Hanmante\\anaconda3\\envs\\idd_detector\\lib\\site-packages\\timm\\models\\_manipulate.py:38\u001b[0m, in \u001b[0;36mnamed_apply\u001b[1;34m(fn, module, name, depth_first, include_root)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child_name, child_module \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[0;32m     37\u001b[0m     child_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin((name, child_name)) \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;28;01melse\u001b[39;00m child_name\n\u001b[1;32m---> 38\u001b[0m     \u001b[43mnamed_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchild_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchild_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdepth_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m depth_first \u001b[38;5;129;01mand\u001b[39;00m include_root:\n\u001b[0;32m     40\u001b[0m     fn(module\u001b[38;5;241m=\u001b[39mmodule, name\u001b[38;5;241m=\u001b[39mname)\n",
      "    \u001b[1;31m[... skipping similar frames: named_apply at line 38 (1 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Sejal Hanmante\\anaconda3\\envs\\idd_detector\\lib\\site-packages\\timm\\models\\_manipulate.py:38\u001b[0m, in \u001b[0;36mnamed_apply\u001b[1;34m(fn, module, name, depth_first, include_root)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child_name, child_module \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[0;32m     37\u001b[0m     child_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin((name, child_name)) \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;28;01melse\u001b[39;00m child_name\n\u001b[1;32m---> 38\u001b[0m     \u001b[43mnamed_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchild_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchild_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdepth_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m depth_first \u001b[38;5;129;01mand\u001b[39;00m include_root:\n\u001b[0;32m     40\u001b[0m     fn(module\u001b[38;5;241m=\u001b[39mmodule, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32mc:\\Users\\Sejal Hanmante\\anaconda3\\envs\\idd_detector\\lib\\site-packages\\timm\\models\\_manipulate.py:40\u001b[0m, in \u001b[0;36mnamed_apply\u001b[1;34m(fn, module, name, depth_first, include_root)\u001b[0m\n\u001b[0;32m     38\u001b[0m     named_apply(fn\u001b[38;5;241m=\u001b[39mfn, module\u001b[38;5;241m=\u001b[39mchild_module, name\u001b[38;5;241m=\u001b[39mchild_name, depth_first\u001b[38;5;241m=\u001b[39mdepth_first, include_root\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m depth_first \u001b[38;5;129;01mand\u001b[39;00m include_root:\n\u001b[1;32m---> 40\u001b[0m     \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "File \u001b[1;32mc:\\Users\\Sejal Hanmante\\anaconda3\\envs\\idd_detector\\lib\\site-packages\\timm\\models\\vision_transformer.py:861\u001b[0m, in \u001b[0;36minit_weights_vit_timm\u001b[1;34m(module, name)\u001b[0m\n\u001b[0;32m    859\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" ViT weight initialization, original timm impl (for reproducibility) \"\"\"\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, nn\u001b[38;5;241m.\u001b[39mLinear):\n\u001b[1;32m--> 861\u001b[0m     \u001b[43mtrunc_normal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.02\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    863\u001b[0m         nn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mzeros_(module\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32mc:\\Users\\Sejal Hanmante\\anaconda3\\envs\\idd_detector\\lib\\site-packages\\timm\\layers\\weight_init.py:67\u001b[0m, in \u001b[0;36mtrunc_normal_\u001b[1;34m(tensor, mean, std, a, b)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Fills the input Tensor with values drawn from a truncated\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03mnormal distribution. The values are effectively drawn from the\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03mnormal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    >>> nn.init.trunc_normal_(w)\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_trunc_normal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sejal Hanmante\\anaconda3\\envs\\idd_detector\\lib\\site-packages\\timm\\layers\\weight_init.py:28\u001b[0m, in \u001b[0;36m_trunc_normal_\u001b[1;34m(tensor, mean, std, a, b)\u001b[0m\n\u001b[0;32m     24\u001b[0m u \u001b[38;5;241m=\u001b[39m norm_cdf((b \u001b[38;5;241m-\u001b[39m mean) \u001b[38;5;241m/\u001b[39m std)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Uniformly fill tensor with values from [l, u], then translate to\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# [2l-1, 2u-1].\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Use inverse cdf transform for normal distribution to get truncated\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# standard normal\u001b[39;00m\n\u001b[0;32m     32\u001b[0m tensor\u001b[38;5;241m.\u001b[39merfinv_()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import math\n",
    "\n",
    "# Load YOLOv8 model\n",
    "yolo_model_path = r\"D:\\NLPInsuranceProject\\AutonomousDrivingGanMV\\idd detection\\shareYolo\\train\\exp12\\weights\\best.pt\"\n",
    "model = YOLO(yolo_model_path)\n",
    "\n",
    "# Load MiDaS depth estimation model from Torch Hub\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", \"DPT_Large\")  # You can use \"MiDaS_small\" for speed\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\").dpt_transform\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "midas.to(device).eval()\n",
    "\n",
    "# Load image\n",
    "image_path = r\"D:\\NLPInsuranceProject\\AutonomousDrivingGanMV\\idd detection\\IDD_FGVD\\train\\images\\8.jpg\"\n",
    "bgr = cv2.imread(image_path)\n",
    "rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Inference with YOLO\n",
    "results = model(rgb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "623080ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use raw NumPy RGB image for MiDaS transform\n",
    "input_rgb_np = rgb / 255.0  # Normalize to 0–1 as expected by MiDaS\n",
    "input_batch = midas_transforms(input_rgb_np).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = midas(input_batch)\n",
    "    prediction = torch.nn.functional.interpolate(\n",
    "        prediction.unsqueeze(1),\n",
    "        size=rgb.shape[:2],\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False\n",
    "    ).squeeze()\n",
    "depth_map = prediction.cpu().numpy()\n",
    "\n",
    "# Normalize depth to meters (Assume max visible depth is ~40 meters)\n",
    "depth_meters = cv2.normalize(depth_map, None, 1, 40, cv2.NORM_MINMAX)\n",
    "\n",
    "# Annotate\n",
    "for result in results:\n",
    "    for box in result.boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "        conf = box.conf[0].item()\n",
    "        class_id = int(box.cls[0].item())\n",
    "\n",
    "        # Get center of box\n",
    "        cx = int((x1 + x2) / 2)\n",
    "        cy = int((y1 + y2) / 2)\n",
    "\n",
    "        # Get estimated distance from camera to object center\n",
    "        est_distance = depth_meters[cy, cx]  # in meters\n",
    "\n",
    "        # Draw bounding box and distance\n",
    "        cv2.rectangle(bgr, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        label = f\"{est_distance:.1f}m\"\n",
    "        cv2.circle(bgr, (cx, cy), 4, (0, 0, 255), -1)\n",
    "        cv2.putText(bgr, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "\n",
    "# Show result\n",
    "cv2.imshow(\"Vehicle Detection with Real-World Distances\", bgr)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c545916",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idd_detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
