{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145c1ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python matplotlib torch torchvision pycocotools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973dddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ultralytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc6d2285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44279612",
   "metadata": {},
   "source": [
    "#### DATASET FOLDER STRUCTURE CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1de5991e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset reorganization complete!\n",
      "Train: 1400 samples\n",
      "Val: 300 samples\n",
      "Test: 300 samples\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configuration\n",
    "original_folders = [r\"D:\\Shivani\\sit\\MV\\MissingTSMini\\task1train540p\", r\"D:\\Shivani\\sit\\MV\\MissingTSMini\\task2train540p\"]  # Your current folders\n",
    "dataset_path = 'dataset'                   # New structured dataset\n",
    "test_size = 0.15                           # 15% for test\n",
    "val_size = 0.15                            # 15% for validation\n",
    "\n",
    "# Create new directory structure\n",
    "os.makedirs(os.path.join(dataset_path, 'train', 'images'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_path, 'train', 'labels'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_path, 'val', 'images'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_path, 'val', 'labels'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_path, 'test', 'images'), exist_ok=True)\n",
    "os.makedirs(os.path.join(dataset_path, 'test', 'labels'), exist_ok=True)\n",
    "\n",
    "# Collect all files\n",
    "all_files = []\n",
    "for folder in original_folders:\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith('.jpg') or file.endswith('.png'):\n",
    "            base_name = os.path.splitext(file)[0]\n",
    "            all_files.append((\n",
    "                os.path.join(folder, file),                 # Image path\n",
    "                os.path.join(folder, f\"{base_name}.txt\")     # Label path\n",
    "            ))\n",
    "\n",
    "# Split into train/val/test\n",
    "train_files, test_files = train_test_split(all_files, test_size=test_size, random_state=42)\n",
    "train_files, val_files = train_test_split(train_files, test_size=val_size/(1-test_size), random_state=42)\n",
    "\n",
    "# Function to copy files\n",
    "def copy_files(file_list, split_name):\n",
    "    for img_path, label_path in file_list:\n",
    "        # Copy image\n",
    "        shutil.copy(\n",
    "            img_path,\n",
    "            os.path.join(dataset_path, split_name, 'images', os.path.basename(img_path))\n",
    "        )\n",
    "        # Copy label\n",
    "        if os.path.exists(label_path):\n",
    "            shutil.copy(\n",
    "                label_path,\n",
    "                os.path.join(dataset_path, split_name, 'labels', os.path.basename(label_path))\n",
    "            )\n",
    "\n",
    "# Execute copying\n",
    "copy_files(train_files, 'train')\n",
    "copy_files(val_files, 'val')\n",
    "copy_files(test_files, 'test')\n",
    "\n",
    "print(\"Dataset reorganization complete!\")\n",
    "print(f\"Train: {len(train_files)} samples\")\n",
    "print(f\"Val: {len(val_files)} samples\")\n",
    "print(f\"Test: {len(test_files)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f8f4f0",
   "metadata": {},
   "source": [
    "#### READING ALL THE CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "342c4c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 unique classes:\n",
      "ID: 3123981 | left-hand-curve      | Samples: 540\n",
      "ID: 4899163 | side-road-left       | Samples: 380\n",
      "ID: 66837424 | right-hand-curve     | Samples: 540\n",
      "ID: 88135902 | gap-in-median        | Samples: 540\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_unique_classes_with_names(dataset_root):\n",
    "    \"\"\"\n",
    "    Extract unique classes with their textual names and counts.\n",
    "    \n",
    "    Args:\n",
    "        dataset_root (str): Path to dataset containing label files\n",
    "        \n",
    "    Returns:\n",
    "        dict: {class_id: {'name': str, 'count': int}}\n",
    "        list: Malformed lines for debugging\n",
    "    \"\"\"\n",
    "    class_info = defaultdict(lambda: {'name': None, 'count': 0})\n",
    "    malformed_lines = []\n",
    "    \n",
    "    for root, _, files in os.walk(dataset_root):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                with open(os.path.join(root, file), 'r') as f:\n",
    "                    for line_num, line in enumerate(f, 1):\n",
    "                        line = line.strip()\n",
    "                        if not line:\n",
    "                            continue\n",
    "                            \n",
    "                        # Parse both formats:\n",
    "                        # 1. \"class_id x y w h\" (YOLO)\n",
    "                        # 2. \"class_name: x,y,w,h\" (IDD)\n",
    "                        if ':' in line:\n",
    "                            try:\n",
    "                                class_part, coords = line.split(':')\n",
    "                                class_name = class_part.strip()\n",
    "                                # Create numeric ID from name hash\n",
    "                                class_id = abs(hash(class_name)) % (10**8)\n",
    "                                class_info[class_id]['name'] = class_name\n",
    "                                class_info[class_id]['count'] += 1\n",
    "                            except ValueError:\n",
    "                                malformed_lines.append(f\"{file}:{line_num} -> {line}\")\n",
    "                        else:\n",
    "                            try:\n",
    "                                parts = line.split()\n",
    "                                class_id = int(parts[0])\n",
    "                                class_info[class_id]['count'] += 1\n",
    "                            except (ValueError, IndexError):\n",
    "                                malformed_lines.append(f\"{file}:{line_num} -> {line}\")\n",
    "    \n",
    "    # Generate report\n",
    "    print(f\"Found {len(class_info)} unique classes:\")\n",
    "    for class_id, info in sorted(class_info.items()):\n",
    "        name = info['name'] or f'unnamed_class_{class_id}'\n",
    "        print(f\"ID: {class_id:3d} | {name:20s} | Samples: {info['count']}\")\n",
    "    \n",
    "    if malformed_lines:\n",
    "        print(\"\\nWarning: Malformed lines detected:\")\n",
    "        for ml in malformed_lines[:5]:  # Print first 5 malformed lines\n",
    "            print(ml)\n",
    "        if len(malformed_lines) > 5:\n",
    "            print(f\"... and {len(malformed_lines)-5} more\")\n",
    "    \n",
    "    return dict(class_info), malformed_lines\n",
    "\n",
    "# Usage\n",
    "class_dict, errors = get_unique_classes_with_names(r'C:\\Users\\DELL\\Documents\\GitHub\\AutonomousDrivingGanMV\\dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cbce54",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c507a2a4",
   "metadata": {},
   "source": [
    "#### Dataset.yaml creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a518045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated dataset.yaml successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "from collections import defaultdict\n",
    "\n",
    "def generate_yaml_config(dataset_root):\n",
    "    class_counts = defaultdict(int)\n",
    "    \n",
    "    # Scan all label files\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        label_dir = os.path.join(dataset_root, split, 'labels')\n",
    "        if os.path.exists(label_dir):\n",
    "            for label_file in os.listdir(label_dir):\n",
    "                if label_file.endswith('.txt'):\n",
    "                    with open(os.path.join(label_dir, label_file), 'r') as f:\n",
    "                        for line in f:\n",
    "                            line = line.strip()\n",
    "                            if ':' in line:\n",
    "                                class_name = line.split(':')[0].strip()\n",
    "                                class_counts[class_name] += 1\n",
    "    \n",
    "    # Create class mapping\n",
    "    names = {i: name for i, name in enumerate(sorted(class_counts.keys()))}\n",
    "    \n",
    "    # Generate YAML\n",
    "    config = {\n",
    "        'path': os.path.abspath(dataset_root),\n",
    "        'train': 'train/images',\n",
    "        'val': 'val/images',\n",
    "        'test': 'test/images',\n",
    "        'names': names,\n",
    "        'nc': len(names),\n",
    "        'stats': {\n",
    "            'class_distribution': dict(sorted(class_counts.items()))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('dataset.yaml', 'w') as f:\n",
    "        yaml.dump(config, f, sort_keys=False)\n",
    "    \n",
    "    print(\"Generated dataset.yaml successfully!\")\n",
    "\n",
    "generate_yaml_config('./dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cb999f",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aed1327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file  \n",
      "View Ultralytics Settings with 'yolo settings' or at 'C:\\Users\\DELL\\AppData\\Roaming\\Ultralytics\\settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8m.pt to 'yolov8m.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49.7M/49.7M [00:12<00:00, 4.31MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.109  Python-3.11.7 torch-2.6.0+cpu \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid CUDA 'device=0' requested. Use 'device=cpu' or pass valid CUDA device(s) if available, i.e. 'device=0' or 'device=0,1,2,3' for Multi-GPU.\n\ntorch.cuda.is_available(): False\ntorch.cuda.device_count(): 0\nos.environ['CUDA_VISIBLE_DEVICES']: None\nSee https://pytorch.org/get-started/locally/ for up-to-date torch install instructions if no CUDA devices are seen by torch.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov8m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# or yolov8s.pt, yolov8m.pt\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDELL\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mGitHub\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAutonomousDrivingGanMV\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdataset.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m , device\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m , verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m , imgsz\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m640\u001b[39m,project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mruns\u001b[39m\u001b[38;5;124m\"\u001b[39m,name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp1\u001b[39m\u001b[38;5;124m\"\u001b[39m,batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m  )\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\model.py:785\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    783\u001b[0m     args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt_path\n\u001b[1;32m--> 785\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m (trainer \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_smart_load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrainer\u001b[39m\u001b[38;5;124m\"\u001b[39m))(overrides\u001b[38;5;241m=\u001b[39margs, _callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks)\n\u001b[0;32m    786\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# manually set model only if not resuming\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mget_model(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39myaml)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:107\u001b[0m, in \u001b[0;36mBaseTrainer.__init__\u001b[1;34m(self, cfg, overrides, _callbacks)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m get_cfg(cfg, overrides)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_resume(overrides)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m select_device(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\ultralytics\\utils\\torch_utils.py:190\u001b[0m, in \u001b[0;36mselect_device\u001b[1;34m(device, batch, newline, verbose)\u001b[0m\n\u001b[0;32m    183\u001b[0m         LOGGER\u001b[38;5;241m.\u001b[39minfo(s)\n\u001b[0;32m    184\u001b[0m         install \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    185\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://pytorch.org/get-started/locally/ for up-to-date torch install instructions if no \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    186\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA devices are seen by torch.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    187\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    188\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    189\u001b[0m         )\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    191\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m requested.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    192\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice=cpu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or pass valid CUDA device(s) if available,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m i.e. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice=0\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice=0,1,2,3\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for Multi-GPU.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    194\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtorch.cuda.is_available(): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtorch.cuda.device_count(): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mos.environ[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvisible\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    197\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstall\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         )\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cpu \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mps \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():  \u001b[38;5;66;03m# prefer GPU if available\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     devices \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# i.e. \"0,1\" -> [\"0\", \"1\"]\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid CUDA 'device=0' requested. Use 'device=cpu' or pass valid CUDA device(s) if available, i.e. 'device=0' or 'device=0,1,2,3' for Multi-GPU.\n\ntorch.cuda.is_available(): False\ntorch.cuda.device_count(): 0\nos.environ['CUDA_VISIBLE_DEVICES']: None\nSee https://pytorch.org/get-started/locally/ for up-to-date torch install instructions if no CUDA devices are seen by torch.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov8m.pt')  # or yolov8s.pt, yolov8m.pt\n",
    "model.train(data=r'C:\\Users\\DELL\\Documents\\GitHub\\AutonomousDrivingGanMV\\dataset.yaml', epochs=10 , device=0 , verbose=True , imgsz=640,project=r\"train\\runs\",name=\"exp1\",batch=16  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7879ba19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
